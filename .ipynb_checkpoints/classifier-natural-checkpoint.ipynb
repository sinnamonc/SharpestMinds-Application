{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "import keras.callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "from data_preprocessing import import_data\n",
    "from utils import shuffle_in_unison\n",
    "from utils import percent_correct\n",
    "from utils import get_uniform_batch\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import the data as a dataframe\n",
    "df = import_data()\n",
    "# print('Dataframe shape:',df.shape)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create test, validation, and training sets\n",
    "test_df = pd.DataFrame()\n",
    "valid_df = pd.DataFrame()\n",
    "train_df = pd.DataFrame()\n",
    "\n",
    "# take about 80% of the data for the training and validation sets\n",
    "train_df_size_per_index = 370000 # about 64% of the data\n",
    "valid_df_size_per_index = 100000 # about 16% of the data\n",
    "\n",
    "#Shuffle the dataframe df\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# Put the first test_df_size into the test set\n",
    "train_df = df[:train_df_size_per_index]\n",
    "# Put the next valid_df_size into the validation set\n",
    "valid_df = df[train_df_size_per_index:train_df_size_per_index+valid_df_size_per_index]\n",
    "# Put the remainder into the training set\n",
    "test_df = df[train_df_size_per_index+valid_df_size_per_index:]\n",
    "\n",
    "# Extract the last columns, which corresponds to the labels\n",
    "test_labels = test_df.iloc[:,-1]\n",
    "valid_labels = valid_df.iloc[:,-1]\n",
    "train_labels = train_df.iloc[:,-1]\n",
    "\n",
    "# Remove the last columns, which corresponds to the labels\n",
    "test_df = test_df.drop(test_df.columns[-1],axis=1)\n",
    "valid_df = valid_df.drop(valid_df.columns[-1],axis=1)\n",
    "train_df = train_df.drop(train_df.columns[-1],axis=1)\n",
    "\n",
    "# Convert data from dataframes to np.arrays\n",
    "test_data = test_df.values\n",
    "valid_data = valid_df.values\n",
    "train_data = train_df.values\n",
    "test_labels = test_labels.values\n",
    "valid_labels = valid_labels.values\n",
    "train_labels = train_labels.values\n",
    "\n",
    "# Convert labels to one hot vectors\n",
    "test_labels = to_categorical(test_labels-1,7)\n",
    "valid_labels = to_categorical(valid_labels-1,7)\n",
    "train_labels = to_categorical(train_labels-1,7)\n",
    "\n",
    "# Shuffle the data and labels\n",
    "shuffle_in_unison(test_data, test_labels)\n",
    "shuffle_in_unison(valid_data, valid_labels)\n",
    "shuffle_in_unison(train_data, train_labels)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Delete existing model\n",
    "del model\n",
    "\n",
    "# Build the model\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Dense(120, activation='relu', input_dim=54))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "optimizer = RMSprop(lr=0.05)\n",
    "# model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "num_epochs_trained = 0\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete.\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, labels, num = 10000):\n",
    "    \"\"\"Gets a batch consisting of num (default 10000) samples randomly chosen with replacement\n",
    "    from the input data and labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    indices = np.random.choice(data.shape[0]-1,num)\n",
    "    batch_data = data[indices,:]\n",
    "    batch_labels = labels[indices,:]\n",
    "    \n",
    "    return batch_data, batch_labels\n",
    "\n",
    "print('Complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2 minutes...\n",
      "Epoch = 69, train_loss = 0.5606, valid_loss = 0.5549, epoch_time = 7.5s"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-d10dfeb17065>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mnum_epochs_trained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_epochs_trained\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mtotal_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m keras.callbacks.ModelCheckpoint('models/classifier-natural.hdf5', monitor='val_loss', verbose=0, \n\u001b[0m\u001b[0;32m     23\u001b[0m                                 save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nTotal time elapsed for this session = {:0.1f}m'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_time\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "# train the model several epochs, and test on the validation set. Plot the loss for train and validation sets\n",
    "# 8 epochs takes about a minute\n",
    "t_init = time.time()\n",
    "# for _ in np.arange(200):\n",
    "train_time = 2\n",
    "print('Training for {} minutes...'.format(train_time))\n",
    "while (time.time() - t_init)/60 < train_time:\n",
    "    #print('Creating batch number', num_epochs_trained + 1, '...')\n",
    "#     batch_data, batch_labels = get_batch(train_data,train_labels)\n",
    "#     if num_epochs_trained%100==0:\n",
    "#         print('Training on batch number', num_epochs_trained + 1, '...')\n",
    "#     train_loss.append(model.train_on_batch(batch_data, batch_labels))\n",
    "    t = time.time()\n",
    "    history = model.fit(train_data, train_labels, batch_size = 1000, epochs = 1, verbose = 0)\n",
    "    train_loss.append(history.history['loss'])\n",
    "    valid_loss.append(model.test_on_batch(valid_data, valid_labels, sample_weight=None))\n",
    "    epoch_time = time.time() - t\n",
    "    print('\\b\\b\\b\\rEpoch = {}, train_loss = {:0.4f}, valid_loss = {:0.4f}, epoch_time = {:0.1f}s'\n",
    "          .format(num_epochs_trained+1, train_loss[-1][0], valid_loss[-1], epoch_time), end='')\n",
    "    num_epochs_trained = num_epochs_trained + 1\n",
    "total_time = time.time() - t_init\n",
    "keras.callbacks.ModelCheckpoint('models/classifier-natural.hdf5', monitor='val_loss', verbose=0, \n",
    "                                save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "print('\\nTotal time elapsed for this session = {:0.1f}m'.format(total_time/60))\n",
    "# print('train_loss =', train_loss[-1], '    valid_loss =', valid_loss[-1])\n",
    "# print('\\nTotal number of epochs trained = {}'.format(num_epochs_trained))\n",
    "\n",
    "# model.save('models/ground_cover_classifier_natural.h5')\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.plot(valid_loss)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ground_cover_classifier_natural.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 78.91534391534391 %\n"
     ]
    }
   ],
   "source": [
    "percent_correct(model,train_data,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 76.95767195767196 %\n"
     ]
    }
   ],
   "source": [
    "percent_correct(model,valid_data,valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete.\n"
     ]
    }
   ],
   "source": [
    "def get_unbalanced_batch_test(data, labels, num_per_label = [100,100,100,100,100,100,6400]):\n",
    "    \"\"\"A function to create a batch with a given number of samples with each label.\"\"\"\n",
    "    full_batch = []\n",
    "    \n",
    "    data_labs = np.concatenate((data,labels),axis=1)\n",
    "    for i in np.arange(0,7):\n",
    "        data_set = data_labs[np.where((labels==to_categorical(i,7)).all(axis=1))[0],:]\n",
    "        indices = np.random.choice(data_set.shape[0]-1,num_per_label[i])\n",
    "        full_batch.extend(data_set[indices,:])\n",
    "\n",
    "    full_batch = np.array(full_batch).reshape(np.sum(num_per_label),data_set.shape[1])\n",
    "    \n",
    "    batch_data = full_batch[:,:-7]\n",
    "    batch_labels = full_batch[:,-7:]\n",
    "    \n",
    "    shuffle_in_unison(batch_data,batch_labels)\n",
    "    \n",
    "    return batch_data, batch_labels\n",
    "\n",
    "print('Complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.320671\n"
     ]
    }
   ],
   "source": [
    "# Show result on test data\n",
    "batch_data, batch_labels = get_unbalanced_batch_test(test_data,test_labels)\n",
    "test_loss = model.test_on_batch(batch_data, batch_labels, sample_weight=None)\n",
    "print(test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 85.41428571428571 %\n"
     ]
    }
   ],
   "source": [
    "percent_correct(model,batch_data,batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
