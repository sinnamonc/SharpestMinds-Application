{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "import keras.initializers\n",
    "import keras.callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "from data_preprocessing import import_data\n",
    "from utils import shuffle_in_unison\n",
    "from utils import percent_correct\n",
    "from utils import get_uniform_batch\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the data as a dataframe\n",
    "df = import_data()\n",
    "# print('Dataframe shape:',df.shape)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create test, validation, and training sets\n",
    "test_df = pd.DataFrame()\n",
    "valid_df = pd.DataFrame()\n",
    "train_df = pd.DataFrame()\n",
    "\n",
    "# take about 80% of the data for the training and validation sets\n",
    "train_df_size_per_index = 370000    # about 64% of the data\n",
    "valid_df_size_per_index = 100000    # about 16% of the data\n",
    "\n",
    "#Shuffle the dataframe df\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# Put the first test_df_size into the test set\n",
    "train_df = df[:train_df_size_per_index]\n",
    "# Put the next valid_df_size into the validation set\n",
    "valid_df = df[train_df_size_per_index:train_df_size_per_index+valid_df_size_per_index]\n",
    "# Put the remainder into the training set\n",
    "test_df = df[train_df_size_per_index+valid_df_size_per_index:]\n",
    "\n",
    "# Extract the last columns, which corresponds to the labels\n",
    "test_labels = test_df.iloc[:, -1]\n",
    "valid_labels = valid_df.iloc[:, -1]\n",
    "train_labels = train_df.iloc[:, -1]\n",
    "\n",
    "# Remove the last columns, which corresponds to the labels\n",
    "test_df = test_df.drop(test_df.columns[-1], axis=1)\n",
    "valid_df = valid_df.drop(valid_df.columns[-1], axis=1)\n",
    "train_df = train_df.drop(train_df.columns[-1], axis=1)\n",
    "\n",
    "# Convert data from dataframes to np.arrays\n",
    "test_data = test_df.values\n",
    "valid_data = valid_df.values\n",
    "train_data = train_df.values\n",
    "test_labels = test_labels.values\n",
    "valid_labels = valid_labels.values\n",
    "train_labels = train_labels.values\n",
    "\n",
    "# Convert labels to one hot vectors\n",
    "test_labels = to_categorical(test_labels-1, 7)\n",
    "valid_labels = to_categorical(valid_labels-1, 7)\n",
    "train_labels = to_categorical(train_labels-1, 7)\n",
    "\n",
    "# Shuffle the data and labels\n",
    "shuffle_in_unison(test_data, test_labels)\n",
    "shuffle_in_unison(valid_data, valid_labels)\n",
    "shuffle_in_unison(train_data, train_labels)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-8f306050411f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Delete existing model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model5' is not defined"
     ]
    }
   ],
   "source": [
    "# Delete existing model\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Build the model - this model acheived a validation loss of about 0.17 after about 600 epochs\n",
    "# print('Build model...')\n",
    "# model = Sequential()\n",
    "# model.add(Dense(240, activation='relu', input_dim=54))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(120, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(60, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "# optimizer = RMSprop(lr=0.05)\n",
    "# # model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "# print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Build the model - this model acheived a validation loss of about 0.12 after about 250 epochs\n",
    "# print('Build model...')\n",
    "# model = Sequential()\n",
    "# model.add(Dense(500, activation='relu', input_dim=54))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(250, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(120, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(60, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "# optimizer = RMSprop(lr=0.05)\n",
    "# # model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "# print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Build the model - validation loss of 0.12 (hits 0.1199 at some point) after 250 epochs, but should probably \n",
    "# stop a good bit sooner by \n",
    "# the look of the plot. Maybe averaging a bunch of these together will give some improvement. Also, look into\n",
    "# parameter optimization.\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=54))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "# model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "num_epochs_trained = 0\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "print('Complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 370000 samples, validate on 100000 samples\n",
      "Epoch 1/250\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(10000, 54), b.shape=(54, 512), m=10000, n=512, k=54\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_arg_dense_1_input_0_1/_189, dense_1/kernel/read)]]\n\t [[Node: loss/mul/_259 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2987_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'dense_1/MatMul', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/craig/.local/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/craig/.local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/craig/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/craig/.local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/craig/.local/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/craig/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/craig/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/craig/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/craig/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/craig/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/craig/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/craig/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/craig/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/craig/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/craig/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/craig/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/craig/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/craig/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-c1f424311272>\", line 7, in <module>\n    model.add(Dense(512, activation='relu', input_dim=54))\n  File \"/home/craig/.local/lib/python3.5/site-packages/keras/models.py\", line 464, in add\n    layer(x)\n  File \"/home/craig/.local/lib/python3.5/site-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/craig/.local/lib/python3.5/site-packages/keras/layers/core.py\", line 843, in call\n    output = K.dot(inputs, self.kernel)\n  File \"/home/craig/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1052, in dot\n    out = tf.matmul(x, y)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 1816, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1217, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(10000, 54), b.shape=(54, 512), m=10000, n=512, k=54\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_arg_dense_1_input_0_1/_189, dense_1/kernel/read)]]\n\t [[Node: loss/mul/_259 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2987_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(10000, 54), b.shape=(54, 512), m=10000, n=512, k=54\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_arg_dense_1_input_0_1/_189, dense_1/kernel/read)]]\n\t [[Node: loss/mul/_259 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2987_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7c533fe08b3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m history = model.fit(train_data, train_labels, batch_size = 10000, epochs = 250, \n\u001b[0;32m---> 15\u001b[0;31m                     validation_data=(valid_data,valid_labels), verbose = 1, callbacks=[early_stopping])\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# train_loss.append(history.history['loss'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# valid_loss.append(model.test_on_batch(valid_data, valid_labels, sample_weight=None))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(10000, 54), b.shape=(54, 512), m=10000, n=512, k=54\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_arg_dense_1_input_0_1/_189, dense_1/kernel/read)]]\n\t [[Node: loss/mul/_259 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2987_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'dense_1/MatMul', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/craig/.local/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/craig/.local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/craig/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/craig/.local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/craig/.local/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/craig/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/craig/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/craig/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/craig/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/craig/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/craig/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/craig/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/craig/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/craig/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/craig/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/craig/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/craig/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/craig/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-c1f424311272>\", line 7, in <module>\n    model.add(Dense(512, activation='relu', input_dim=54))\n  File \"/home/craig/.local/lib/python3.5/site-packages/keras/models.py\", line 464, in add\n    layer(x)\n  File \"/home/craig/.local/lib/python3.5/site-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/craig/.local/lib/python3.5/site-packages/keras/layers/core.py\", line 843, in call\n    output = K.dot(inputs, self.kernel)\n  File \"/home/craig/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1052, in dot\n    out = tf.matmul(x, y)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 1816, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1217, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(10000, 54), b.shape=(54, 512), m=10000, n=512, k=54\n\t [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_arg_dense_1_input_0_1/_189, dense_1/kernel/read)]]\n\t [[Node: loss/mul/_259 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2987_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# train the model several epochs, and test on the validation set. Plot the loss for train and validation sets\n",
    "t_init = time.time()\n",
    "# for _ in np.arange(200):\n",
    "# train_time = 15 # how long to train for in minutes\n",
    "# print('Training for {} minutes...'.format(train_time))\n",
    "# while (time.time() - t_init)/60 < train_time:\n",
    "    #print('Creating batch number', num_epochs_trained + 1, '...')\n",
    "#     batch_data, batch_labels = get_batch(train_data,train_labels)\n",
    "#     if num_epochs_trained%100==0:\n",
    "#         print('Training on batch number', num_epochs_trained + 1, '...')\n",
    "#     train_loss.append(model.train_on_batch(batch_data, batch_labels))\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "t = time.time()\n",
    "history = model.fit(train_data, train_labels, batch_size=10000, epochs=250, \n",
    "                    validation_data=(valid_data,valid_labels), verbose=1, callbacks=[early_stopping])\n",
    "# train_loss.append(history.history['loss'])\n",
    "# valid_loss.append(model.test_on_batch(valid_data, valid_labels, sample_weight=None))\n",
    "epoch_time = time.time() - t\n",
    "# print('\\b\\b\\b\\rEpoch = {}, train_loss = {:0.4f}, valid_loss = {:0.4f}, epoch_time = {:0.1f}s, current_time = {:0.1f}m'\n",
    "#       .format(num_epochs_trained+1, train_loss[-1][0], valid_loss[-1], epoch_time, (time.time()-t_init)/60), end='')\n",
    "num_epochs_trained = num_epochs_trained + 1\n",
    "total_time = time.time() - t_init\n",
    "\n",
    "# print('\\nTotal time elapsed for this session = {:0.1f}m'.format(total_time/60))\n",
    "# print('train_loss =', train_loss[-1], '    valid_loss =', valid_loss[-1])\n",
    "# print('\\nTotal number of epochs trained = {}'.format(num_epochs_trained))\n",
    "\n",
    "# model.save('models/ground_cover_classifier_natural.h5')\n",
    "\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"acc\"])\n",
    "plt.plot(history.history[\"val_acc\"])\n",
    "plt.title('model acc')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = 'models/ground_cover_classifier_natural.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-39bf05068a52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/ground_cover_classifier_natural.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   2554\u001b[0m         \"\"\"\n\u001b[1;32m   2555\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2556\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keras_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'backend'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = 'models/ground_cover_classifier_natural.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "model.save('models/ground_cover_classifier_natural.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 98.28216216216217 %\n"
     ]
    }
   ],
   "source": [
    "percent_correct(model, train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 96.004 %\n"
     ]
    }
   ],
   "source": [
    "percent_correct(model, valid_data, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss= 0.126042\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.test_on_batch(test_data, test_labels, sample_weight=None)\n",
    "print('test_loss=', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 95.89503837422981 %\n"
     ]
    }
   ],
   "source": [
    "percent_correct(model, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Build the model - validation loss of 0.12 (hits 0.1199 at some point) after 250 epochs, but should probably \n",
    "# stop a good bit sooner by \n",
    "# the look of the plot. Maybe averaging a bunch of these together will give some improvement. Also, look into\n",
    "# parameter optimization.\n",
    "print('Build model...')\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(512, activation='relu', input_dim=54))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(256, activation='relu'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(128, activation='relu'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(64, activation='relu'))\n",
    "model1.add(BatchNormalization())\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model1.add(Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "# model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "model1.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 370000 samples, validate on 100000 samples\n",
      "Epoch 1/2500\n",
      "370000/370000 [==============================] - 6s 16us/step - loss: 0.6259 - acc: 0.7662 - val_loss: 0.9620 - val_acc: 0.5906\n",
      "Epoch 2/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.4287 - acc: 0.8238 - val_loss: 0.4120 - val_acc: 0.8285\n",
      "Epoch 3/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.3659 - acc: 0.8517 - val_loss: 0.3516 - val_acc: 0.8577\n",
      "Epoch 4/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.3238 - acc: 0.8680 - val_loss: 0.3062 - val_acc: 0.8740\n",
      "Epoch 5/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2878 - acc: 0.8819 - val_loss: 0.3027 - val_acc: 0.8760\n",
      "Epoch 6/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2662 - acc: 0.8911 - val_loss: 0.2758 - val_acc: 0.8844\n",
      "Epoch 7/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2495 - acc: 0.8985 - val_loss: 0.2569 - val_acc: 0.8958\n",
      "Epoch 8/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2349 - acc: 0.9043 - val_loss: 0.2469 - val_acc: 0.8988\n",
      "Epoch 9/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.2238 - acc: 0.9084 - val_loss: 0.2396 - val_acc: 0.9028\n",
      "Epoch 10/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2141 - acc: 0.9126 - val_loss: 0.2206 - val_acc: 0.9099\n",
      "Epoch 11/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2054 - acc: 0.9164 - val_loss: 0.2195 - val_acc: 0.9110\n",
      "Epoch 12/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1989 - acc: 0.9187 - val_loss: 0.2099 - val_acc: 0.9145\n",
      "Epoch 13/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1920 - acc: 0.9214 - val_loss: 0.2086 - val_acc: 0.9155\n",
      "Epoch 14/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1864 - acc: 0.9240 - val_loss: 0.2059 - val_acc: 0.9161\n",
      "Epoch 15/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1811 - acc: 0.9260 - val_loss: 0.1950 - val_acc: 0.9211\n",
      "Epoch 16/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1767 - acc: 0.9278 - val_loss: 0.1965 - val_acc: 0.9209\n",
      "Epoch 17/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1707 - acc: 0.9300 - val_loss: 0.1958 - val_acc: 0.9219\n",
      "Epoch 18/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1659 - acc: 0.9330 - val_loss: 0.1959 - val_acc: 0.9207\n",
      "Epoch 19/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1630 - acc: 0.9337 - val_loss: 0.1876 - val_acc: 0.9245\n",
      "Epoch 20/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1602 - acc: 0.9348 - val_loss: 0.1716 - val_acc: 0.9313\n",
      "Epoch 21/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1578 - acc: 0.9357 - val_loss: 0.1819 - val_acc: 0.9270\n",
      "Epoch 22/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1546 - acc: 0.9367 - val_loss: 0.1730 - val_acc: 0.9310\n",
      "Epoch 23/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1510 - acc: 0.9384 - val_loss: 0.1832 - val_acc: 0.9264\n",
      "Epoch 24/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1486 - acc: 0.9393 - val_loss: 0.1666 - val_acc: 0.9335\n",
      "Epoch 25/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1464 - acc: 0.9403 - val_loss: 0.1846 - val_acc: 0.9260\n",
      "Epoch 26/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1446 - acc: 0.9415 - val_loss: 0.1695 - val_acc: 0.9322\n",
      "Epoch 27/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1413 - acc: 0.9427 - val_loss: 0.1682 - val_acc: 0.9332\n",
      "Epoch 28/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1408 - acc: 0.9429 - val_loss: 0.1653 - val_acc: 0.9340\n",
      "Epoch 29/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1381 - acc: 0.9441 - val_loss: 0.1530 - val_acc: 0.9399\n",
      "Epoch 30/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1355 - acc: 0.9449 - val_loss: 0.1598 - val_acc: 0.9364\n",
      "Epoch 31/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1332 - acc: 0.9459 - val_loss: 0.1562 - val_acc: 0.9374\n",
      "Epoch 32/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1321 - acc: 0.9464 - val_loss: 0.1665 - val_acc: 0.9345\n",
      "Epoch 33/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1318 - acc: 0.9465 - val_loss: 0.1567 - val_acc: 0.9378\n",
      "Epoch 34/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1298 - acc: 0.9477 - val_loss: 0.1492 - val_acc: 0.9402\n",
      "Epoch 35/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1269 - acc: 0.9483 - val_loss: 0.1646 - val_acc: 0.9348\n",
      "Epoch 36/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1274 - acc: 0.9486 - val_loss: 0.1646 - val_acc: 0.9359\n",
      "Epoch 37/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1246 - acc: 0.9494 - val_loss: 0.1536 - val_acc: 0.9392\n",
      "Epoch 38/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1245 - acc: 0.9493 - val_loss: 0.1490 - val_acc: 0.9414\n",
      "Epoch 39/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1221 - acc: 0.9506 - val_loss: 0.1493 - val_acc: 0.9410\n",
      "Epoch 40/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1206 - acc: 0.9506 - val_loss: 0.1471 - val_acc: 0.9423\n",
      "Epoch 41/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1184 - acc: 0.9520 - val_loss: 0.1445 - val_acc: 0.9432\n",
      "Epoch 42/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1182 - acc: 0.9522 - val_loss: 0.1411 - val_acc: 0.9452\n",
      "Epoch 43/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1171 - acc: 0.9522 - val_loss: 0.1468 - val_acc: 0.9422\n",
      "Epoch 44/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1149 - acc: 0.9533 - val_loss: 0.1446 - val_acc: 0.9429\n",
      "Epoch 45/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1150 - acc: 0.9534 - val_loss: 0.1468 - val_acc: 0.9429\n",
      "Epoch 46/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1139 - acc: 0.9538 - val_loss: 0.1375 - val_acc: 0.9468\n",
      "Epoch 47/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1130 - acc: 0.9543 - val_loss: 0.1446 - val_acc: 0.9442\n",
      "Epoch 48/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1116 - acc: 0.9547 - val_loss: 0.1465 - val_acc: 0.9426\n",
      "Epoch 49/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1109 - acc: 0.9550 - val_loss: 0.1434 - val_acc: 0.9448\n",
      "Epoch 50/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1097 - acc: 0.9552 - val_loss: 0.1386 - val_acc: 0.9454\n",
      "Epoch 51/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1089 - acc: 0.9557 - val_loss: 0.1480 - val_acc: 0.9413\n",
      "Epoch 52/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1083 - acc: 0.9558 - val_loss: 0.1412 - val_acc: 0.9456\n",
      "Epoch 53/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1079 - acc: 0.9562 - val_loss: 0.1478 - val_acc: 0.9423\n",
      "Epoch 54/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1060 - acc: 0.9570 - val_loss: 0.1404 - val_acc: 0.9460\n",
      "Epoch 55/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1058 - acc: 0.9569 - val_loss: 0.1460 - val_acc: 0.9434\n",
      "Epoch 56/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1046 - acc: 0.9577 - val_loss: 0.1354 - val_acc: 0.9474\n",
      "Epoch 57/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1043 - acc: 0.9578 - val_loss: 0.1448 - val_acc: 0.9451\n",
      "Epoch 58/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1037 - acc: 0.9580 - val_loss: 0.1364 - val_acc: 0.9475\n",
      "Epoch 59/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1017 - acc: 0.9589 - val_loss: 0.1346 - val_acc: 0.9475\n",
      "Epoch 60/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1013 - acc: 0.9586 - val_loss: 0.1442 - val_acc: 0.9443\n",
      "Epoch 61/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1008 - acc: 0.9588 - val_loss: 0.1622 - val_acc: 0.9377\n",
      "Epoch 62/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1001 - acc: 0.9590 - val_loss: 0.1417 - val_acc: 0.9463\n",
      "Epoch 63/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0998 - acc: 0.9595 - val_loss: 0.1315 - val_acc: 0.9489\n",
      "Epoch 64/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0983 - acc: 0.9603 - val_loss: 0.1286 - val_acc: 0.9504\n",
      "Epoch 65/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0982 - acc: 0.9603 - val_loss: 0.1296 - val_acc: 0.9505\n",
      "Epoch 66/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0976 - acc: 0.9602 - val_loss: 0.1349 - val_acc: 0.9486\n",
      "Epoch 67/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0967 - acc: 0.9604 - val_loss: 0.1323 - val_acc: 0.9500\n",
      "Epoch 68/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0962 - acc: 0.9610 - val_loss: 0.1329 - val_acc: 0.9495\n",
      "Epoch 69/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0949 - acc: 0.9616 - val_loss: 0.1277 - val_acc: 0.9512\n",
      "Epoch 70/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0948 - acc: 0.9616 - val_loss: 0.1367 - val_acc: 0.9475\n",
      "Epoch 71/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0949 - acc: 0.9615 - val_loss: 0.1291 - val_acc: 0.9512\n",
      "Epoch 72/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0935 - acc: 0.9624 - val_loss: 0.1313 - val_acc: 0.9504\n",
      "Epoch 73/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0922 - acc: 0.9625 - val_loss: 0.1325 - val_acc: 0.9494\n",
      "Epoch 74/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0921 - acc: 0.9624 - val_loss: 0.1332 - val_acc: 0.9499\n",
      "Epoch 75/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0917 - acc: 0.9627 - val_loss: 0.1304 - val_acc: 0.9508\n",
      "Epoch 76/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0918 - acc: 0.9626 - val_loss: 0.1316 - val_acc: 0.9508\n",
      "Epoch 77/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0912 - acc: 0.9628 - val_loss: 0.1305 - val_acc: 0.9516\n",
      "Epoch 78/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0903 - acc: 0.9635 - val_loss: 0.1236 - val_acc: 0.9533\n",
      "Epoch 79/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0888 - acc: 0.9642 - val_loss: 0.1374 - val_acc: 0.9483\n",
      "Epoch 80/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0894 - acc: 0.9639 - val_loss: 0.1327 - val_acc: 0.9494\n",
      "Epoch 81/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0877 - acc: 0.9644 - val_loss: 0.1326 - val_acc: 0.9507\n",
      "Epoch 82/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0891 - acc: 0.9640 - val_loss: 0.1238 - val_acc: 0.9536\n",
      "Epoch 83/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0877 - acc: 0.9644 - val_loss: 0.1310 - val_acc: 0.9502\n",
      "Epoch 84/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0885 - acc: 0.9639 - val_loss: 0.1290 - val_acc: 0.9512\n",
      "Epoch 85/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0873 - acc: 0.9644 - val_loss: 0.1406 - val_acc: 0.9483\n",
      "Epoch 86/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0867 - acc: 0.9650 - val_loss: 0.1243 - val_acc: 0.9534\n",
      "Epoch 87/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0854 - acc: 0.9653 - val_loss: 0.1281 - val_acc: 0.9526\n",
      "Epoch 88/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0853 - acc: 0.9655 - val_loss: 0.1341 - val_acc: 0.9509\n",
      "Epoch 89/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0854 - acc: 0.9654 - val_loss: 0.1276 - val_acc: 0.9533\n",
      "Epoch 90/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0842 - acc: 0.9656 - val_loss: 0.1265 - val_acc: 0.9535\n",
      "Epoch 91/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0845 - acc: 0.9658 - val_loss: 0.1234 - val_acc: 0.9548\n",
      "Epoch 92/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0836 - acc: 0.9664 - val_loss: 0.1281 - val_acc: 0.9517\n",
      "Epoch 93/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0830 - acc: 0.9663 - val_loss: 0.1288 - val_acc: 0.9516\n",
      "Epoch 94/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0830 - acc: 0.9663 - val_loss: 0.1381 - val_acc: 0.9485\n",
      "Epoch 95/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0837 - acc: 0.9664 - val_loss: 0.1257 - val_acc: 0.9542\n",
      "Epoch 96/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0821 - acc: 0.9668 - val_loss: 0.1299 - val_acc: 0.9522\n",
      "Epoch 97/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0821 - acc: 0.9669 - val_loss: 0.1307 - val_acc: 0.9522\n",
      "Epoch 98/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0815 - acc: 0.9670 - val_loss: 0.1298 - val_acc: 0.9527\n",
      "Epoch 99/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0807 - acc: 0.9674 - val_loss: 0.1169 - val_acc: 0.9570\n",
      "Epoch 100/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0803 - acc: 0.9676 - val_loss: 0.1261 - val_acc: 0.9538\n",
      "Epoch 101/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0803 - acc: 0.9675 - val_loss: 0.1347 - val_acc: 0.9510\n",
      "Epoch 102/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0811 - acc: 0.9672 - val_loss: 0.1311 - val_acc: 0.9520\n",
      "Epoch 103/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0798 - acc: 0.9676 - val_loss: 0.1277 - val_acc: 0.9537\n",
      "Epoch 104/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0786 - acc: 0.9680 - val_loss: 0.1290 - val_acc: 0.9527\n",
      "Epoch 105/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0805 - acc: 0.9673 - val_loss: 0.1222 - val_acc: 0.9547\n",
      "Epoch 106/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0786 - acc: 0.9682 - val_loss: 0.1325 - val_acc: 0.9515\n",
      "Epoch 107/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0779 - acc: 0.9685 - val_loss: 0.1252 - val_acc: 0.9543\n",
      "Epoch 108/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0781 - acc: 0.9684 - val_loss: 0.1294 - val_acc: 0.9529\n",
      "Epoch 109/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0779 - acc: 0.9686 - val_loss: 0.1273 - val_acc: 0.9537\n",
      "Epoch 110/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0770 - acc: 0.9689 - val_loss: 0.1217 - val_acc: 0.9557\n",
      "Epoch 111/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0781 - acc: 0.9685 - val_loss: 0.1236 - val_acc: 0.9542\n",
      "Epoch 112/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0769 - acc: 0.9691 - val_loss: 0.1202 - val_acc: 0.9561\n",
      "Epoch 113/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0763 - acc: 0.9687 - val_loss: 0.1205 - val_acc: 0.9562\n",
      "Epoch 114/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0754 - acc: 0.9699 - val_loss: 0.1256 - val_acc: 0.9549\n",
      "Epoch 115/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0764 - acc: 0.9693 - val_loss: 0.1387 - val_acc: 0.9501\n",
      "Epoch 116/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0761 - acc: 0.9694 - val_loss: 0.1211 - val_acc: 0.9559\n",
      "Epoch 117/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0759 - acc: 0.9694 - val_loss: 0.1167 - val_acc: 0.9575\n",
      "Epoch 118/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0755 - acc: 0.9695 - val_loss: 0.1273 - val_acc: 0.9539\n",
      "Epoch 119/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0749 - acc: 0.9701 - val_loss: 0.1170 - val_acc: 0.9571\n",
      "Epoch 120/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0733 - acc: 0.9703 - val_loss: 0.1201 - val_acc: 0.9573\n",
      "Epoch 121/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0740 - acc: 0.9700 - val_loss: 0.1296 - val_acc: 0.9538\n",
      "Epoch 122/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0740 - acc: 0.9700 - val_loss: 0.1215 - val_acc: 0.9555\n",
      "Epoch 123/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0741 - acc: 0.9699 - val_loss: 0.1263 - val_acc: 0.9548\n",
      "Epoch 124/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0754 - acc: 0.9696 - val_loss: 0.1255 - val_acc: 0.9548\n",
      "Epoch 125/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0728 - acc: 0.9706 - val_loss: 0.1227 - val_acc: 0.9563\n",
      "Epoch 126/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0741 - acc: 0.9700 - val_loss: 0.1219 - val_acc: 0.9557\n",
      "Epoch 127/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0725 - acc: 0.9707 - val_loss: 0.1213 - val_acc: 0.9560\n",
      "Epoch 128/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0721 - acc: 0.9709 - val_loss: 0.1229 - val_acc: 0.9552\n",
      "Epoch 129/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0724 - acc: 0.9708 - val_loss: 0.1218 - val_acc: 0.9566\n",
      "Epoch 130/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0718 - acc: 0.9709 - val_loss: 0.1203 - val_acc: 0.9570\n",
      "Epoch 131/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0710 - acc: 0.9712 - val_loss: 0.1263 - val_acc: 0.9550\n",
      "Epoch 132/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0722 - acc: 0.9705 - val_loss: 0.1318 - val_acc: 0.9532\n",
      "Epoch 133/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0705 - acc: 0.9715 - val_loss: 0.1243 - val_acc: 0.9558\n",
      "Epoch 134/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0705 - acc: 0.9714 - val_loss: 0.1242 - val_acc: 0.9560\n",
      "Epoch 135/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0699 - acc: 0.9715 - val_loss: 0.1242 - val_acc: 0.9556\n",
      "Epoch 136/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0705 - acc: 0.9714 - val_loss: 0.1277 - val_acc: 0.9542\n",
      "Epoch 137/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0694 - acc: 0.9717 - val_loss: 0.1227 - val_acc: 0.9562\n",
      "Epoch 138/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0693 - acc: 0.9722 - val_loss: 0.1242 - val_acc: 0.9559\n",
      "Epoch 139/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0689 - acc: 0.9725 - val_loss: 0.1187 - val_acc: 0.9579\n",
      "Epoch 140/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0697 - acc: 0.9719 - val_loss: 0.1216 - val_acc: 0.9571\n",
      "Epoch 141/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0685 - acc: 0.9722 - val_loss: 0.1198 - val_acc: 0.9572\n",
      "Epoch 142/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0690 - acc: 0.9721 - val_loss: 0.1355 - val_acc: 0.9522\n",
      "Epoch 143/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0687 - acc: 0.9723 - val_loss: 0.1204 - val_acc: 0.9573\n",
      "Epoch 144/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0684 - acc: 0.9723 - val_loss: 0.1192 - val_acc: 0.9576\n",
      "Epoch 145/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0680 - acc: 0.9727 - val_loss: 0.1296 - val_acc: 0.9549\n",
      "Epoch 146/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0669 - acc: 0.9730 - val_loss: 0.1228 - val_acc: 0.9559\n",
      "Epoch 147/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0684 - acc: 0.9722 - val_loss: 0.1224 - val_acc: 0.9567\n",
      "Epoch 148/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0679 - acc: 0.9727 - val_loss: 0.1192 - val_acc: 0.9582\n",
      "Epoch 149/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0663 - acc: 0.9730 - val_loss: 0.1208 - val_acc: 0.9576\n",
      "Epoch 150/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0670 - acc: 0.9729 - val_loss: 0.1226 - val_acc: 0.9566\n",
      "Epoch 151/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0666 - acc: 0.9730 - val_loss: 0.1201 - val_acc: 0.9578\n",
      "Epoch 152/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0674 - acc: 0.9728 - val_loss: 0.1173 - val_acc: 0.9584\n",
      "Epoch 153/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0655 - acc: 0.9735 - val_loss: 0.1280 - val_acc: 0.9557\n",
      "Epoch 154/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0662 - acc: 0.9731 - val_loss: 0.1225 - val_acc: 0.9571\n",
      "Epoch 155/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0670 - acc: 0.9729 - val_loss: 0.1158 - val_acc: 0.9592\n",
      "Epoch 156/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0651 - acc: 0.9737 - val_loss: 0.1215 - val_acc: 0.9571\n",
      "Epoch 157/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0656 - acc: 0.9737 - val_loss: 0.1217 - val_acc: 0.9575\n",
      "Epoch 158/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0654 - acc: 0.9737 - val_loss: 0.1217 - val_acc: 0.9572\n",
      "Epoch 159/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0651 - acc: 0.9736 - val_loss: 0.1186 - val_acc: 0.9586\n",
      "Epoch 160/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0644 - acc: 0.9743 - val_loss: 0.1196 - val_acc: 0.9583\n",
      "Epoch 161/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0650 - acc: 0.9734 - val_loss: 0.1158 - val_acc: 0.9592\n",
      "Epoch 162/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0642 - acc: 0.9741 - val_loss: 0.1181 - val_acc: 0.9579\n",
      "Epoch 163/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0653 - acc: 0.9737 - val_loss: 0.1251 - val_acc: 0.9566\n",
      "Epoch 164/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0643 - acc: 0.9741 - val_loss: 0.1183 - val_acc: 0.9590\n",
      "Epoch 165/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0636 - acc: 0.9745 - val_loss: 0.1244 - val_acc: 0.9567\n",
      "Epoch 166/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0649 - acc: 0.9741 - val_loss: 0.1151 - val_acc: 0.9601\n",
      "Epoch 167/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0627 - acc: 0.9746 - val_loss: 0.1186 - val_acc: 0.9590\n",
      "Epoch 168/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0641 - acc: 0.9741 - val_loss: 0.1218 - val_acc: 0.9583\n",
      "Epoch 169/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0636 - acc: 0.9745 - val_loss: 0.1212 - val_acc: 0.9578\n",
      "Epoch 170/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0630 - acc: 0.9747 - val_loss: 0.1240 - val_acc: 0.9569\n",
      "Epoch 171/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0633 - acc: 0.9744 - val_loss: 0.1183 - val_acc: 0.9585\n",
      "Epoch 172/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0624 - acc: 0.9748 - val_loss: 0.1236 - val_acc: 0.9579\n",
      "Epoch 173/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0640 - acc: 0.9740 - val_loss: 0.1180 - val_acc: 0.9588\n",
      "Epoch 174/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0630 - acc: 0.9747 - val_loss: 0.1188 - val_acc: 0.9586\n",
      "Epoch 175/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0623 - acc: 0.9747 - val_loss: 0.1271 - val_acc: 0.9568\n",
      "Epoch 176/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0621 - acc: 0.9748 - val_loss: 0.1233 - val_acc: 0.9573\n",
      "Epoch 177/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0624 - acc: 0.9747 - val_loss: 0.1209 - val_acc: 0.9582\n",
      "Epoch 178/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0615 - acc: 0.9752 - val_loss: 0.1239 - val_acc: 0.9576\n",
      "Epoch 179/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0616 - acc: 0.9751 - val_loss: 0.1261 - val_acc: 0.9574\n",
      "Epoch 180/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0619 - acc: 0.9753 - val_loss: 0.1212 - val_acc: 0.9583\n",
      "Epoch 181/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0614 - acc: 0.9754 - val_loss: 0.1217 - val_acc: 0.9581\n",
      "Epoch 182/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0613 - acc: 0.9755 - val_loss: 0.1209 - val_acc: 0.9590\n",
      "Epoch 183/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0611 - acc: 0.9757 - val_loss: 0.1210 - val_acc: 0.9586\n",
      "Epoch 184/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0603 - acc: 0.9759 - val_loss: 0.1228 - val_acc: 0.9581\n",
      "Epoch 185/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0611 - acc: 0.9755 - val_loss: 0.1179 - val_acc: 0.9594\n",
      "Epoch 186/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0608 - acc: 0.9755 - val_loss: 0.1215 - val_acc: 0.9585\n",
      "Epoch 187/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0600 - acc: 0.9758 - val_loss: 0.1203 - val_acc: 0.9592\n",
      "Epoch 188/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0595 - acc: 0.9763 - val_loss: 0.1216 - val_acc: 0.9585\n",
      "Epoch 189/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0612 - acc: 0.9756 - val_loss: 0.1207 - val_acc: 0.9593\n",
      "Epoch 190/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0601 - acc: 0.9758 - val_loss: 0.1285 - val_acc: 0.9569\n",
      "Epoch 191/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0602 - acc: 0.9756 - val_loss: 0.1218 - val_acc: 0.9587\n",
      "Epoch 192/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0596 - acc: 0.9757 - val_loss: 0.1277 - val_acc: 0.9570\n",
      "Epoch 193/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0588 - acc: 0.9763 - val_loss: 0.1250 - val_acc: 0.9583\n",
      "Epoch 194/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0593 - acc: 0.9762 - val_loss: 0.1229 - val_acc: 0.9586\n",
      "Epoch 195/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0596 - acc: 0.9763 - val_loss: 0.1158 - val_acc: 0.9600\n",
      "Epoch 196/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0591 - acc: 0.9759 - val_loss: 0.1220 - val_acc: 0.9591\n",
      "Epoch 197/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0582 - acc: 0.9767 - val_loss: 0.1208 - val_acc: 0.9597\n",
      "Epoch 198/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0579 - acc: 0.9767 - val_loss: 0.1301 - val_acc: 0.9554\n",
      "Epoch 199/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0595 - acc: 0.9762 - val_loss: 0.1221 - val_acc: 0.9590\n",
      "Epoch 200/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0580 - acc: 0.9766 - val_loss: 0.1202 - val_acc: 0.9593\n",
      "Epoch 201/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0584 - acc: 0.9765 - val_loss: 0.1282 - val_acc: 0.9566\n",
      "Epoch 202/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0590 - acc: 0.9762 - val_loss: 0.1205 - val_acc: 0.9602\n",
      "Epoch 203/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0588 - acc: 0.9765 - val_loss: 0.1204 - val_acc: 0.9597\n",
      "Epoch 204/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0575 - acc: 0.9768 - val_loss: 0.1275 - val_acc: 0.9569\n",
      "Epoch 205/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0583 - acc: 0.9766 - val_loss: 0.1229 - val_acc: 0.9591\n",
      "Epoch 206/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0573 - acc: 0.9772 - val_loss: 0.1175 - val_acc: 0.9606\n",
      "Epoch 207/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0571 - acc: 0.9769 - val_loss: 0.1224 - val_acc: 0.9588\n",
      "Epoch 208/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0577 - acc: 0.9766 - val_loss: 0.1158 - val_acc: 0.9607\n",
      "Epoch 209/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0573 - acc: 0.9772 - val_loss: 0.1194 - val_acc: 0.9598\n",
      "Epoch 210/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0574 - acc: 0.9769 - val_loss: 0.1207 - val_acc: 0.9591\n",
      "Epoch 211/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0569 - acc: 0.9771 - val_loss: 0.1214 - val_acc: 0.9590\n",
      "Epoch 212/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0566 - acc: 0.9773 - val_loss: 0.1177 - val_acc: 0.9609\n",
      "Epoch 213/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0569 - acc: 0.9773 - val_loss: 0.1250 - val_acc: 0.9582\n",
      "Epoch 214/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0563 - acc: 0.9771 - val_loss: 0.1186 - val_acc: 0.9602\n",
      "Epoch 215/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0563 - acc: 0.9771 - val_loss: 0.1220 - val_acc: 0.9594\n",
      "Epoch 216/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0573 - acc: 0.9771 - val_loss: 0.1229 - val_acc: 0.9585\n",
      "Epoch 217/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0552 - acc: 0.9778 - val_loss: 0.1205 - val_acc: 0.9598\n",
      "Epoch 218/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0563 - acc: 0.9773 - val_loss: 0.1231 - val_acc: 0.9593\n",
      "Epoch 219/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0561 - acc: 0.9777 - val_loss: 0.1215 - val_acc: 0.9590\n",
      "Epoch 220/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0564 - acc: 0.9773 - val_loss: 0.1194 - val_acc: 0.9599\n",
      "Epoch 221/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0552 - acc: 0.9777 - val_loss: 0.1246 - val_acc: 0.9588\n",
      "Epoch 222/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0552 - acc: 0.9778 - val_loss: 0.1209 - val_acc: 0.9599\n",
      "Epoch 223/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0564 - acc: 0.9775 - val_loss: 0.1283 - val_acc: 0.9581\n",
      "Epoch 224/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0562 - acc: 0.9774 - val_loss: 0.1276 - val_acc: 0.9577\n",
      "Epoch 225/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0546 - acc: 0.9781 - val_loss: 0.1202 - val_acc: 0.9606\n",
      "Epoch 226/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0548 - acc: 0.9780 - val_loss: 0.1254 - val_acc: 0.9586\n",
      "Epoch 227/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0552 - acc: 0.9778 - val_loss: 0.1217 - val_acc: 0.9600\n",
      "Epoch 228/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0547 - acc: 0.9782 - val_loss: 0.1315 - val_acc: 0.9576\n",
      "Epoch 229/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0549 - acc: 0.9782 - val_loss: 0.1231 - val_acc: 0.9593\n",
      "Epoch 230/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0549 - acc: 0.9778 - val_loss: 0.1195 - val_acc: 0.9595\n",
      "Epoch 231/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0539 - acc: 0.9784 - val_loss: 0.1188 - val_acc: 0.9609\n",
      "Epoch 232/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0543 - acc: 0.9782 - val_loss: 0.1248 - val_acc: 0.9591\n",
      "Epoch 233/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0542 - acc: 0.9784 - val_loss: 0.1163 - val_acc: 0.9624\n",
      "Epoch 234/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0546 - acc: 0.9782 - val_loss: 0.1227 - val_acc: 0.9596\n",
      "Epoch 235/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0537 - acc: 0.9783 - val_loss: 0.1213 - val_acc: 0.9603\n",
      "Epoch 236/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0536 - acc: 0.9785 - val_loss: 0.1229 - val_acc: 0.9603\n",
      "Epoch 237/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0538 - acc: 0.9784 - val_loss: 0.1225 - val_acc: 0.9595\n",
      "Epoch 238/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0538 - acc: 0.9785 - val_loss: 0.1270 - val_acc: 0.9586\n",
      "Epoch 239/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0537 - acc: 0.9785 - val_loss: 0.1187 - val_acc: 0.9615\n",
      "Epoch 240/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0531 - acc: 0.9787 - val_loss: 0.1185 - val_acc: 0.9615\n",
      "Epoch 241/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0524 - acc: 0.9788 - val_loss: 0.1204 - val_acc: 0.9604\n",
      "Epoch 242/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0533 - acc: 0.9783 - val_loss: 0.1210 - val_acc: 0.9598\n",
      "Epoch 243/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0532 - acc: 0.9787 - val_loss: 0.1279 - val_acc: 0.9587\n",
      "Epoch 244/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0531 - acc: 0.9787 - val_loss: 0.1219 - val_acc: 0.9607\n",
      "Epoch 245/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0541 - acc: 0.9783 - val_loss: 0.1225 - val_acc: 0.9599\n",
      "Epoch 246/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0537 - acc: 0.9784 - val_loss: 0.1229 - val_acc: 0.9596\n",
      "Epoch 247/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0528 - acc: 0.9788 - val_loss: 0.1223 - val_acc: 0.9599\n",
      "Epoch 248/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0525 - acc: 0.9791 - val_loss: 0.1213 - val_acc: 0.9604\n",
      "Epoch 249/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0527 - acc: 0.9787 - val_loss: 0.1250 - val_acc: 0.9595\n",
      "Epoch 250/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0523 - acc: 0.9788 - val_loss: 0.1225 - val_acc: 0.9602\n",
      "Epoch 251/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0523 - acc: 0.9790 - val_loss: 0.1226 - val_acc: 0.9600\n",
      "Epoch 252/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0513 - acc: 0.9795 - val_loss: 0.1160 - val_acc: 0.9624\n",
      "Epoch 253/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0527 - acc: 0.9791 - val_loss: 0.1218 - val_acc: 0.9604\n",
      "Epoch 254/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0520 - acc: 0.9791 - val_loss: 0.1208 - val_acc: 0.9600\n",
      "Epoch 255/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0516 - acc: 0.9791 - val_loss: 0.1235 - val_acc: 0.9603\n",
      "Epoch 256/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0528 - acc: 0.9788 - val_loss: 0.1209 - val_acc: 0.9607\n",
      "Epoch 257/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0518 - acc: 0.9793 - val_loss: 0.1222 - val_acc: 0.9609\n",
      "Epoch 258/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0520 - acc: 0.9792 - val_loss: 0.1213 - val_acc: 0.9611\n",
      "Epoch 259/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0509 - acc: 0.9798 - val_loss: 0.1219 - val_acc: 0.9605\n",
      "Epoch 260/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0512 - acc: 0.9797 - val_loss: 0.1263 - val_acc: 0.9586\n",
      "Epoch 261/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0513 - acc: 0.9796 - val_loss: 0.1213 - val_acc: 0.9606\n",
      "Epoch 262/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0507 - acc: 0.9795 - val_loss: 0.1270 - val_acc: 0.9580\n",
      "Epoch 263/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0515 - acc: 0.9795 - val_loss: 0.1239 - val_acc: 0.9595\n",
      "Epoch 264/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0522 - acc: 0.9793 - val_loss: 0.1238 - val_acc: 0.9594\n",
      "Epoch 265/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0501 - acc: 0.9800 - val_loss: 0.1168 - val_acc: 0.9622\n",
      "Epoch 266/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0516 - acc: 0.9793 - val_loss: 0.1198 - val_acc: 0.9619\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niLT5f0qS/EL9YqB6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9685c36860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFiIikpeShYiI5KVkISIief1/j6fKT10cSRgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9685a65630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model several epochs, and test on the validation set. Plot the loss for train and validation sets\n",
    "t_init = time.time()\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "t = time.time()\n",
    "history = model1.fit(train_data, train_labels, batch_size=1000, epochs=2500, \n",
    "                    validation_data=(valid_data, valid_labels), verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "epoch_time = time.time() - t\n",
    "\n",
    "total_time = time.time() - t_init\n",
    "\n",
    "\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"acc\"])\n",
    "plt.plot(history.history[\"val_acc\"])\n",
    "plt.title('model acc')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Build the model - validation loss of 0.12 (hits 0.1199 at some point) after 250 epochs, but should probably \n",
    "# stop a good bit sooner by \n",
    "# the look of the plot. Maybe averaging a bunch of these together will give some improvement. Also, look into\n",
    "# parameter optimization.\n",
    "print('Build model...')\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(512, activation='relu', input_dim=54))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dense(256, activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "# model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 370000 samples, validate on 100000 samples\n",
      "Epoch 1/2500\n",
      "370000/370000 [==============================] - 6s 16us/step - loss: 0.6268 - acc: 0.7683 - val_loss: 0.8363 - val_acc: 0.6665\n",
      "Epoch 2/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.4245 - acc: 0.8249 - val_loss: 0.4095 - val_acc: 0.8271\n",
      "Epoch 3/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.3607 - acc: 0.8533 - val_loss: 0.3461 - val_acc: 0.8617\n",
      "Epoch 4/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.3200 - acc: 0.8712 - val_loss: 0.3109 - val_acc: 0.8721\n",
      "Epoch 5/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2862 - acc: 0.8825 - val_loss: 0.2843 - val_acc: 0.8826\n",
      "Epoch 6/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2630 - acc: 0.8925 - val_loss: 0.2625 - val_acc: 0.8923\n",
      "Epoch 7/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2477 - acc: 0.8989 - val_loss: 0.2515 - val_acc: 0.8963\n",
      "Epoch 8/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2343 - acc: 0.9049 - val_loss: 0.2559 - val_acc: 0.8946\n",
      "Epoch 9/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2217 - acc: 0.9097 - val_loss: 0.2421 - val_acc: 0.9025\n",
      "Epoch 10/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2136 - acc: 0.9131 - val_loss: 0.2323 - val_acc: 0.9058\n",
      "Epoch 11/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2062 - acc: 0.9158 - val_loss: 0.2141 - val_acc: 0.9139\n",
      "Epoch 12/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1992 - acc: 0.9188 - val_loss: 0.2163 - val_acc: 0.9126\n",
      "Epoch 13/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1925 - acc: 0.9215 - val_loss: 0.2180 - val_acc: 0.9102\n",
      "Epoch 14/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1875 - acc: 0.9234 - val_loss: 0.2062 - val_acc: 0.9161\n",
      "Epoch 15/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1815 - acc: 0.9258 - val_loss: 0.2000 - val_acc: 0.9203\n",
      "Epoch 16/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1767 - acc: 0.9282 - val_loss: 0.1981 - val_acc: 0.9190\n",
      "Epoch 17/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1736 - acc: 0.9294 - val_loss: 0.1872 - val_acc: 0.9253\n",
      "Epoch 18/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1694 - acc: 0.9312 - val_loss: 0.1902 - val_acc: 0.9226\n",
      "Epoch 19/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1652 - acc: 0.9328 - val_loss: 0.1873 - val_acc: 0.9243\n",
      "Epoch 20/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1620 - acc: 0.9342 - val_loss: 0.1783 - val_acc: 0.9274\n",
      "Epoch 21/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1581 - acc: 0.9358 - val_loss: 0.1864 - val_acc: 0.9247\n",
      "Epoch 22/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1563 - acc: 0.9365 - val_loss: 0.1751 - val_acc: 0.9298\n",
      "Epoch 23/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1530 - acc: 0.9378 - val_loss: 0.1766 - val_acc: 0.9297\n",
      "Epoch 24/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1508 - acc: 0.9384 - val_loss: 0.1642 - val_acc: 0.9342\n",
      "Epoch 25/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1480 - acc: 0.9397 - val_loss: 0.1721 - val_acc: 0.9313\n",
      "Epoch 26/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1461 - acc: 0.9408 - val_loss: 0.1672 - val_acc: 0.9335\n",
      "Epoch 27/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1429 - acc: 0.9424 - val_loss: 0.1692 - val_acc: 0.9332\n",
      "Epoch 28/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1419 - acc: 0.9420 - val_loss: 0.1651 - val_acc: 0.9352\n",
      "Epoch 29/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1393 - acc: 0.9436 - val_loss: 0.1703 - val_acc: 0.9316\n",
      "Epoch 30/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1374 - acc: 0.9442 - val_loss: 0.1607 - val_acc: 0.9365\n",
      "Epoch 31/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1352 - acc: 0.9452 - val_loss: 0.1616 - val_acc: 0.9361\n",
      "Epoch 32/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1331 - acc: 0.9459 - val_loss: 0.1501 - val_acc: 0.9412\n",
      "Epoch 33/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1320 - acc: 0.9463 - val_loss: 0.1567 - val_acc: 0.9377\n",
      "Epoch 34/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1315 - acc: 0.9468 - val_loss: 0.1596 - val_acc: 0.9368\n",
      "Epoch 35/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1281 - acc: 0.9479 - val_loss: 0.1500 - val_acc: 0.9405\n",
      "Epoch 36/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1271 - acc: 0.9481 - val_loss: 0.1568 - val_acc: 0.9380\n",
      "Epoch 37/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1259 - acc: 0.9490 - val_loss: 0.1610 - val_acc: 0.9362\n",
      "Epoch 38/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1241 - acc: 0.9497 - val_loss: 0.1503 - val_acc: 0.9407\n",
      "Epoch 39/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1227 - acc: 0.9502 - val_loss: 0.1478 - val_acc: 0.9417\n",
      "Epoch 40/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1219 - acc: 0.9504 - val_loss: 0.1434 - val_acc: 0.9434\n",
      "Epoch 41/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1210 - acc: 0.9509 - val_loss: 0.1458 - val_acc: 0.9431\n",
      "Epoch 42/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1193 - acc: 0.9519 - val_loss: 0.1509 - val_acc: 0.9414\n",
      "Epoch 43/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1181 - acc: 0.9521 - val_loss: 0.1518 - val_acc: 0.9404\n",
      "Epoch 44/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1173 - acc: 0.9521 - val_loss: 0.1522 - val_acc: 0.9411\n",
      "Epoch 45/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1164 - acc: 0.9530 - val_loss: 0.1424 - val_acc: 0.9435\n",
      "Epoch 46/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1154 - acc: 0.9532 - val_loss: 0.1422 - val_acc: 0.9449\n",
      "Epoch 47/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1139 - acc: 0.9537 - val_loss: 0.1468 - val_acc: 0.9430\n",
      "Epoch 48/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1140 - acc: 0.9535 - val_loss: 0.1442 - val_acc: 0.9441\n",
      "Epoch 49/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1130 - acc: 0.9546 - val_loss: 0.1453 - val_acc: 0.9438\n",
      "Epoch 50/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1113 - acc: 0.9546 - val_loss: 0.1468 - val_acc: 0.9436\n",
      "Epoch 51/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1096 - acc: 0.9557 - val_loss: 0.1398 - val_acc: 0.9463\n",
      "Epoch 52/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1098 - acc: 0.9552 - val_loss: 0.1379 - val_acc: 0.9465\n",
      "Epoch 53/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1088 - acc: 0.9556 - val_loss: 0.1399 - val_acc: 0.9455\n",
      "Epoch 54/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1082 - acc: 0.9562 - val_loss: 0.1352 - val_acc: 0.9476\n",
      "Epoch 55/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1072 - acc: 0.9568 - val_loss: 0.1375 - val_acc: 0.9476\n",
      "Epoch 56/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1058 - acc: 0.9570 - val_loss: 0.1437 - val_acc: 0.9440\n",
      "Epoch 57/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1049 - acc: 0.9573 - val_loss: 0.1504 - val_acc: 0.9425\n",
      "Epoch 58/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1038 - acc: 0.9578 - val_loss: 0.1341 - val_acc: 0.9485\n",
      "Epoch 59/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1043 - acc: 0.9581 - val_loss: 0.1380 - val_acc: 0.9469\n",
      "Epoch 60/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1026 - acc: 0.9581 - val_loss: 0.1404 - val_acc: 0.9459\n",
      "Epoch 61/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1027 - acc: 0.9583 - val_loss: 0.1380 - val_acc: 0.9472\n",
      "Epoch 62/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1018 - acc: 0.9587 - val_loss: 0.1279 - val_acc: 0.9507\n",
      "Epoch 63/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1014 - acc: 0.9589 - val_loss: 0.1371 - val_acc: 0.9474\n",
      "Epoch 64/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0999 - acc: 0.9596 - val_loss: 0.1360 - val_acc: 0.9485\n",
      "Epoch 65/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0982 - acc: 0.9601 - val_loss: 0.1332 - val_acc: 0.9492\n",
      "Epoch 66/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0991 - acc: 0.9597 - val_loss: 0.1398 - val_acc: 0.9473\n",
      "Epoch 67/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0993 - acc: 0.9600 - val_loss: 0.1307 - val_acc: 0.9501\n",
      "Epoch 68/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0967 - acc: 0.9609 - val_loss: 0.1309 - val_acc: 0.9507\n",
      "Epoch 69/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0974 - acc: 0.9607 - val_loss: 0.1289 - val_acc: 0.9506\n",
      "Epoch 70/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0958 - acc: 0.9611 - val_loss: 0.1278 - val_acc: 0.9508\n",
      "Epoch 71/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0956 - acc: 0.9612 - val_loss: 0.1310 - val_acc: 0.9502\n",
      "Epoch 72/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0952 - acc: 0.9617 - val_loss: 0.1375 - val_acc: 0.9481\n",
      "Epoch 73/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0945 - acc: 0.9619 - val_loss: 0.1306 - val_acc: 0.9501\n",
      "Epoch 74/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0936 - acc: 0.9620 - val_loss: 0.1265 - val_acc: 0.9524\n",
      "Epoch 75/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0937 - acc: 0.9625 - val_loss: 0.1318 - val_acc: 0.9506\n",
      "Epoch 76/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0927 - acc: 0.9623 - val_loss: 0.1297 - val_acc: 0.9502\n",
      "Epoch 77/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0938 - acc: 0.9618 - val_loss: 0.1277 - val_acc: 0.9513\n",
      "Epoch 78/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0905 - acc: 0.9634 - val_loss: 0.1372 - val_acc: 0.9484\n",
      "Epoch 79/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0909 - acc: 0.9630 - val_loss: 0.1467 - val_acc: 0.9450\n",
      "Epoch 80/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0913 - acc: 0.9627 - val_loss: 0.1366 - val_acc: 0.9487\n",
      "Epoch 81/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0913 - acc: 0.9630 - val_loss: 0.1381 - val_acc: 0.9482\n",
      "Epoch 82/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0901 - acc: 0.9638 - val_loss: 0.1266 - val_acc: 0.9526\n",
      "Epoch 83/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0887 - acc: 0.9641 - val_loss: 0.1267 - val_acc: 0.9526\n",
      "Epoch 84/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0894 - acc: 0.9638 - val_loss: 0.1291 - val_acc: 0.9523\n",
      "Epoch 85/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0884 - acc: 0.9643 - val_loss: 0.1305 - val_acc: 0.9510\n",
      "Epoch 86/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0876 - acc: 0.9646 - val_loss: 0.1234 - val_acc: 0.9538\n",
      "Epoch 87/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0870 - acc: 0.9647 - val_loss: 0.1267 - val_acc: 0.9523\n",
      "Epoch 88/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0878 - acc: 0.9645 - val_loss: 0.1294 - val_acc: 0.9514\n",
      "Epoch 89/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0865 - acc: 0.9649 - val_loss: 0.1301 - val_acc: 0.9518\n",
      "Epoch 90/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0867 - acc: 0.9647 - val_loss: 0.1277 - val_acc: 0.9521\n",
      "Epoch 91/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0848 - acc: 0.9654 - val_loss: 0.1330 - val_acc: 0.9500\n",
      "Epoch 92/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0855 - acc: 0.9653 - val_loss: 0.1250 - val_acc: 0.9533\n",
      "Epoch 93/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0852 - acc: 0.9654 - val_loss: 0.1248 - val_acc: 0.9529\n",
      "Epoch 94/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0843 - acc: 0.9660 - val_loss: 0.1243 - val_acc: 0.9536\n",
      "Epoch 95/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0842 - acc: 0.9661 - val_loss: 0.1281 - val_acc: 0.9531\n",
      "Epoch 96/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0833 - acc: 0.9662 - val_loss: 0.1258 - val_acc: 0.9532\n",
      "Epoch 97/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0829 - acc: 0.9667 - val_loss: 0.1230 - val_acc: 0.9538\n",
      "Epoch 98/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0831 - acc: 0.9662 - val_loss: 0.1238 - val_acc: 0.9537\n",
      "Epoch 99/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0827 - acc: 0.9668 - val_loss: 0.1223 - val_acc: 0.9543\n",
      "Epoch 100/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0823 - acc: 0.9670 - val_loss: 0.1319 - val_acc: 0.9507\n",
      "Epoch 101/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0829 - acc: 0.9666 - val_loss: 0.1192 - val_acc: 0.9565\n",
      "Epoch 102/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0812 - acc: 0.9671 - val_loss: 0.1165 - val_acc: 0.9567\n",
      "Epoch 103/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0815 - acc: 0.9668 - val_loss: 0.1212 - val_acc: 0.9548\n",
      "Epoch 104/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0801 - acc: 0.9676 - val_loss: 0.1225 - val_acc: 0.9549\n",
      "Epoch 105/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0810 - acc: 0.9670 - val_loss: 0.1411 - val_acc: 0.9477\n",
      "Epoch 106/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0803 - acc: 0.9673 - val_loss: 0.1228 - val_acc: 0.9553\n",
      "Epoch 107/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0797 - acc: 0.9676 - val_loss: 0.1288 - val_acc: 0.9528\n",
      "Epoch 108/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0802 - acc: 0.9676 - val_loss: 0.1241 - val_acc: 0.9541\n",
      "Epoch 109/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0792 - acc: 0.9681 - val_loss: 0.1254 - val_acc: 0.9533\n",
      "Epoch 110/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0790 - acc: 0.9679 - val_loss: 0.1235 - val_acc: 0.9551\n",
      "Epoch 111/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0767 - acc: 0.9687 - val_loss: 0.1304 - val_acc: 0.9520\n",
      "Epoch 112/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0787 - acc: 0.9683 - val_loss: 0.1247 - val_acc: 0.9540\n",
      "Epoch 113/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0771 - acc: 0.9690 - val_loss: 0.1206 - val_acc: 0.9559\n",
      "Epoch 114/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0783 - acc: 0.9683 - val_loss: 0.1207 - val_acc: 0.9553\n",
      "Epoch 115/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0780 - acc: 0.9685 - val_loss: 0.1209 - val_acc: 0.9563\n",
      "Epoch 116/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0770 - acc: 0.9692 - val_loss: 0.1247 - val_acc: 0.9546\n",
      "Epoch 117/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0760 - acc: 0.9691 - val_loss: 0.1230 - val_acc: 0.9545\n",
      "Epoch 118/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0760 - acc: 0.9694 - val_loss: 0.1339 - val_acc: 0.9511\n",
      "Epoch 119/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0757 - acc: 0.9692 - val_loss: 0.1264 - val_acc: 0.9538\n",
      "Epoch 120/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0768 - acc: 0.9692 - val_loss: 0.1254 - val_acc: 0.9541\n",
      "Epoch 121/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0762 - acc: 0.9692 - val_loss: 0.1368 - val_acc: 0.9504\n",
      "Epoch 122/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0746 - acc: 0.9702 - val_loss: 0.1347 - val_acc: 0.9511\n",
      "Epoch 123/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0755 - acc: 0.9695 - val_loss: 0.1228 - val_acc: 0.9556\n",
      "Epoch 124/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0747 - acc: 0.9699 - val_loss: 0.1178 - val_acc: 0.9572\n",
      "Epoch 125/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0738 - acc: 0.9703 - val_loss: 0.1204 - val_acc: 0.9567\n",
      "Epoch 126/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0741 - acc: 0.9703 - val_loss: 0.1219 - val_acc: 0.9563\n",
      "Epoch 127/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0738 - acc: 0.9702 - val_loss: 0.1259 - val_acc: 0.9545\n",
      "Epoch 128/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0739 - acc: 0.9700 - val_loss: 0.1202 - val_acc: 0.9567\n",
      "Epoch 129/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0740 - acc: 0.9701 - val_loss: 0.1262 - val_acc: 0.9544\n",
      "Epoch 130/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0727 - acc: 0.9704 - val_loss: 0.1240 - val_acc: 0.9551\n",
      "Epoch 131/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0728 - acc: 0.9708 - val_loss: 0.1177 - val_acc: 0.9582\n",
      "Epoch 132/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0715 - acc: 0.9712 - val_loss: 0.1205 - val_acc: 0.9572\n",
      "Epoch 133/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0715 - acc: 0.9713 - val_loss: 0.1200 - val_acc: 0.9567\n",
      "Epoch 134/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0721 - acc: 0.9709 - val_loss: 0.1203 - val_acc: 0.9569\n",
      "Epoch 135/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0716 - acc: 0.9712 - val_loss: 0.1212 - val_acc: 0.9567\n",
      "Epoch 136/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0717 - acc: 0.9712 - val_loss: 0.1242 - val_acc: 0.9552\n",
      "Epoch 137/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0716 - acc: 0.9712 - val_loss: 0.1156 - val_acc: 0.9581\n",
      "Epoch 138/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0711 - acc: 0.9715 - val_loss: 0.1240 - val_acc: 0.9551\n",
      "Epoch 139/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0713 - acc: 0.9713 - val_loss: 0.1252 - val_acc: 0.9547\n",
      "Epoch 140/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0704 - acc: 0.9714 - val_loss: 0.1223 - val_acc: 0.9567\n",
      "Epoch 141/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0702 - acc: 0.9716 - val_loss: 0.1231 - val_acc: 0.9567\n",
      "Epoch 142/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0706 - acc: 0.9717 - val_loss: 0.1183 - val_acc: 0.9577\n",
      "Epoch 143/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0698 - acc: 0.9718 - val_loss: 0.1218 - val_acc: 0.9568\n",
      "Epoch 144/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0699 - acc: 0.9719 - val_loss: 0.1226 - val_acc: 0.9567\n",
      "Epoch 145/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0693 - acc: 0.9721 - val_loss: 0.1195 - val_acc: 0.9569\n",
      "Epoch 146/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0697 - acc: 0.9717 - val_loss: 0.1250 - val_acc: 0.9562\n",
      "Epoch 147/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0683 - acc: 0.9723 - val_loss: 0.1213 - val_acc: 0.9565\n",
      "Epoch 148/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0684 - acc: 0.9725 - val_loss: 0.1201 - val_acc: 0.9575\n",
      "Epoch 149/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0685 - acc: 0.9724 - val_loss: 0.1265 - val_acc: 0.9550\n",
      "Epoch 150/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0685 - acc: 0.9722 - val_loss: 0.1291 - val_acc: 0.9543\n",
      "Epoch 151/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0692 - acc: 0.9721 - val_loss: 0.1276 - val_acc: 0.9545\n",
      "Epoch 152/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0676 - acc: 0.9728 - val_loss: 0.1193 - val_acc: 0.9569\n",
      "Epoch 153/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0676 - acc: 0.9728 - val_loss: 0.1177 - val_acc: 0.9585\n",
      "Epoch 154/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0678 - acc: 0.9727 - val_loss: 0.1230 - val_acc: 0.9565\n",
      "Epoch 155/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0674 - acc: 0.9727 - val_loss: 0.1230 - val_acc: 0.9573\n",
      "Epoch 156/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0669 - acc: 0.9728 - val_loss: 0.1214 - val_acc: 0.9571\n",
      "Epoch 157/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0667 - acc: 0.9732 - val_loss: 0.1195 - val_acc: 0.9583\n",
      "Epoch 158/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0675 - acc: 0.9727 - val_loss: 0.1191 - val_acc: 0.9573\n",
      "Epoch 159/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0666 - acc: 0.9733 - val_loss: 0.1281 - val_acc: 0.9552\n",
      "Epoch 160/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0667 - acc: 0.9735 - val_loss: 0.1220 - val_acc: 0.9572\n",
      "Epoch 161/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0667 - acc: 0.9732 - val_loss: 0.1243 - val_acc: 0.9561\n",
      "Epoch 162/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0651 - acc: 0.9737 - val_loss: 0.1196 - val_acc: 0.9581\n",
      "Epoch 163/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0666 - acc: 0.9730 - val_loss: 0.1178 - val_acc: 0.9583\n",
      "Epoch 164/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0652 - acc: 0.9737 - val_loss: 0.1219 - val_acc: 0.9571\n",
      "Epoch 165/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0661 - acc: 0.9733 - val_loss: 0.1231 - val_acc: 0.9567\n",
      "Epoch 166/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0654 - acc: 0.9735 - val_loss: 0.1193 - val_acc: 0.9588\n",
      "Epoch 167/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0643 - acc: 0.9739 - val_loss: 0.1194 - val_acc: 0.9588\n",
      "Epoch 168/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0644 - acc: 0.9741 - val_loss: 0.1196 - val_acc: 0.9580\n",
      "Epoch 169/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0649 - acc: 0.9739 - val_loss: 0.1235 - val_acc: 0.9567\n",
      "Epoch 170/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0652 - acc: 0.9736 - val_loss: 0.1225 - val_acc: 0.9577\n",
      "Epoch 171/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0641 - acc: 0.9743 - val_loss: 0.1221 - val_acc: 0.9572\n",
      "Epoch 172/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0644 - acc: 0.9739 - val_loss: 0.1160 - val_acc: 0.9595\n",
      "Epoch 173/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0637 - acc: 0.9743 - val_loss: 0.1228 - val_acc: 0.9580\n",
      "Epoch 174/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0637 - acc: 0.9742 - val_loss: 0.1212 - val_acc: 0.9589\n",
      "Epoch 175/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0636 - acc: 0.9743 - val_loss: 0.1251 - val_acc: 0.9569\n",
      "Epoch 176/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0634 - acc: 0.9744 - val_loss: 0.1219 - val_acc: 0.9579\n",
      "Epoch 177/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0620 - acc: 0.9751 - val_loss: 0.1243 - val_acc: 0.9576\n",
      "Epoch 178/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0634 - acc: 0.9745 - val_loss: 0.1165 - val_acc: 0.9597\n",
      "Epoch 179/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0630 - acc: 0.9748 - val_loss: 0.1236 - val_acc: 0.9571\n",
      "Epoch 180/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0630 - acc: 0.9748 - val_loss: 0.1237 - val_acc: 0.9574\n",
      "Epoch 181/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0629 - acc: 0.9746 - val_loss: 0.1171 - val_acc: 0.9592\n",
      "Epoch 182/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0627 - acc: 0.9748 - val_loss: 0.1225 - val_acc: 0.9578\n",
      "Epoch 183/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0620 - acc: 0.9747 - val_loss: 0.1164 - val_acc: 0.9592\n",
      "Epoch 184/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0626 - acc: 0.9749 - val_loss: 0.1220 - val_acc: 0.9579\n",
      "Epoch 185/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0615 - acc: 0.9754 - val_loss: 0.1153 - val_acc: 0.9605\n",
      "Epoch 186/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0612 - acc: 0.9757 - val_loss: 0.1207 - val_acc: 0.9589\n",
      "Epoch 187/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0616 - acc: 0.9753 - val_loss: 0.1235 - val_acc: 0.9576\n",
      "Epoch 188/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0616 - acc: 0.9753 - val_loss: 0.1249 - val_acc: 0.9563\n",
      "Epoch 189/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0616 - acc: 0.9752 - val_loss: 0.1210 - val_acc: 0.9593\n",
      "Epoch 190/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0615 - acc: 0.9754 - val_loss: 0.1174 - val_acc: 0.9597\n",
      "Epoch 191/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0604 - acc: 0.9754 - val_loss: 0.1321 - val_acc: 0.9548\n",
      "Epoch 192/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0617 - acc: 0.9753 - val_loss: 0.1199 - val_acc: 0.9588\n",
      "Epoch 193/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0594 - acc: 0.9759 - val_loss: 0.1177 - val_acc: 0.9602\n",
      "Epoch 194/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0603 - acc: 0.9756 - val_loss: 0.1197 - val_acc: 0.9588\n",
      "Epoch 195/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0600 - acc: 0.9758 - val_loss: 0.1164 - val_acc: 0.9603\n",
      "Epoch 196/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0607 - acc: 0.9755 - val_loss: 0.1187 - val_acc: 0.9601\n",
      "Epoch 197/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0604 - acc: 0.9757 - val_loss: 0.1191 - val_acc: 0.9586\n",
      "Epoch 198/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0608 - acc: 0.9755 - val_loss: 0.1279 - val_acc: 0.9562\n",
      "Epoch 199/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0594 - acc: 0.9761 - val_loss: 0.1222 - val_acc: 0.9586\n",
      "Epoch 200/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0597 - acc: 0.9760 - val_loss: 0.1220 - val_acc: 0.9584\n",
      "Epoch 201/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0590 - acc: 0.9761 - val_loss: 0.1188 - val_acc: 0.9592\n",
      "Epoch 202/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0603 - acc: 0.9757 - val_loss: 0.1199 - val_acc: 0.9595\n",
      "Epoch 203/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0596 - acc: 0.9761 - val_loss: 0.1250 - val_acc: 0.9581\n",
      "Epoch 204/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0591 - acc: 0.9762 - val_loss: 0.1219 - val_acc: 0.9584\n",
      "Epoch 205/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0584 - acc: 0.9768 - val_loss: 0.1282 - val_acc: 0.9556\n",
      "Epoch 206/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0594 - acc: 0.9762 - val_loss: 0.1217 - val_acc: 0.9584\n",
      "Epoch 207/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0585 - acc: 0.9764 - val_loss: 0.1158 - val_acc: 0.9610\n",
      "Epoch 208/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0594 - acc: 0.9761 - val_loss: 0.1169 - val_acc: 0.9601\n",
      "Epoch 209/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0579 - acc: 0.9767 - val_loss: 0.1289 - val_acc: 0.9575\n",
      "Epoch 210/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0588 - acc: 0.9764 - val_loss: 0.1249 - val_acc: 0.9573\n",
      "Epoch 211/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0583 - acc: 0.9767 - val_loss: 0.1205 - val_acc: 0.9597\n",
      "Epoch 212/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0579 - acc: 0.9770 - val_loss: 0.1230 - val_acc: 0.9582\n",
      "Epoch 213/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0572 - acc: 0.9772 - val_loss: 0.1161 - val_acc: 0.9610\n",
      "Epoch 214/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0582 - acc: 0.9766 - val_loss: 0.1165 - val_acc: 0.9605\n",
      "Epoch 215/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0580 - acc: 0.9765 - val_loss: 0.1190 - val_acc: 0.9598\n",
      "Epoch 216/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0572 - acc: 0.9770 - val_loss: 0.1228 - val_acc: 0.9585\n",
      "Epoch 217/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0584 - acc: 0.9767 - val_loss: 0.1244 - val_acc: 0.9576\n",
      "Epoch 218/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0573 - acc: 0.9771 - val_loss: 0.1215 - val_acc: 0.9594\n",
      "Epoch 219/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0572 - acc: 0.9773 - val_loss: 0.1163 - val_acc: 0.9609\n",
      "Epoch 220/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0573 - acc: 0.9769 - val_loss: 0.1183 - val_acc: 0.9604\n",
      "Epoch 221/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0569 - acc: 0.9771 - val_loss: 0.1232 - val_acc: 0.9590\n",
      "Epoch 222/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0577 - acc: 0.9769 - val_loss: 0.1242 - val_acc: 0.9575\n",
      "Epoch 223/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0572 - acc: 0.9772 - val_loss: 0.1298 - val_acc: 0.9567\n",
      "Epoch 224/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0576 - acc: 0.9767 - val_loss: 0.1191 - val_acc: 0.9599\n",
      "Epoch 225/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0557 - acc: 0.9777 - val_loss: 0.1209 - val_acc: 0.9596\n",
      "Epoch 226/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0554 - acc: 0.9777 - val_loss: 0.1253 - val_acc: 0.9581\n",
      "Epoch 227/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0557 - acc: 0.9776 - val_loss: 0.1200 - val_acc: 0.9596\n",
      "Epoch 228/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0555 - acc: 0.9776 - val_loss: 0.1210 - val_acc: 0.9597\n",
      "Epoch 229/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0563 - acc: 0.9774 - val_loss: 0.1206 - val_acc: 0.9599\n",
      "Epoch 230/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0553 - acc: 0.9779 - val_loss: 0.1226 - val_acc: 0.9602\n",
      "Epoch 231/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0562 - acc: 0.9775 - val_loss: 0.1225 - val_acc: 0.9591\n",
      "Epoch 232/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0560 - acc: 0.9775 - val_loss: 0.1223 - val_acc: 0.9594\n",
      "Epoch 233/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0557 - acc: 0.9775 - val_loss: 0.1230 - val_acc: 0.9591\n",
      "Epoch 234/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0540 - acc: 0.9782 - val_loss: 0.1196 - val_acc: 0.9601\n",
      "Epoch 235/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0559 - acc: 0.9777 - val_loss: 0.1195 - val_acc: 0.9601\n",
      "Epoch 236/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0553 - acc: 0.9779 - val_loss: 0.1207 - val_acc: 0.9592\n",
      "Epoch 237/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0547 - acc: 0.9781 - val_loss: 0.1204 - val_acc: 0.9604\n",
      "Epoch 238/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0552 - acc: 0.9778 - val_loss: 0.1194 - val_acc: 0.9603\n",
      "Epoch 239/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0546 - acc: 0.9781 - val_loss: 0.1203 - val_acc: 0.9601\n",
      "Epoch 240/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0555 - acc: 0.9781 - val_loss: 0.1233 - val_acc: 0.9597\n",
      "Epoch 241/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0551 - acc: 0.9779 - val_loss: 0.1173 - val_acc: 0.9610\n",
      "Epoch 242/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0541 - acc: 0.9785 - val_loss: 0.1234 - val_acc: 0.9586\n",
      "Epoch 243/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0540 - acc: 0.9783 - val_loss: 0.1174 - val_acc: 0.9610\n",
      "Epoch 244/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0547 - acc: 0.9781 - val_loss: 0.1250 - val_acc: 0.9598\n",
      "Epoch 245/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0552 - acc: 0.9778 - val_loss: 0.1192 - val_acc: 0.9607\n",
      "Epoch 246/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0526 - acc: 0.9789 - val_loss: 0.1212 - val_acc: 0.9602\n",
      "Epoch 247/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0546 - acc: 0.9780 - val_loss: 0.1191 - val_acc: 0.9606\n",
      "Epoch 248/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0533 - acc: 0.9787 - val_loss: 0.1192 - val_acc: 0.9607\n",
      "Epoch 249/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0542 - acc: 0.9781 - val_loss: 0.1226 - val_acc: 0.9601\n",
      "Epoch 250/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0534 - acc: 0.9784 - val_loss: 0.1180 - val_acc: 0.9618\n",
      "Epoch 251/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0531 - acc: 0.9789 - val_loss: 0.1265 - val_acc: 0.9587\n",
      "Epoch 252/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0533 - acc: 0.9788 - val_loss: 0.1233 - val_acc: 0.9598\n",
      "Epoch 253/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0534 - acc: 0.9786 - val_loss: 0.1182 - val_acc: 0.9613\n",
      "Epoch 254/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0525 - acc: 0.9790 - val_loss: 0.1216 - val_acc: 0.9599\n",
      "Epoch 255/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0540 - acc: 0.9783 - val_loss: 0.1184 - val_acc: 0.9618\n",
      "Epoch 256/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0529 - acc: 0.9786 - val_loss: 0.1254 - val_acc: 0.9592\n",
      "Epoch 257/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0534 - acc: 0.9786 - val_loss: 0.1180 - val_acc: 0.9608\n",
      "Epoch 258/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0527 - acc: 0.9787 - val_loss: 0.1156 - val_acc: 0.9624\n",
      "Epoch 259/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0528 - acc: 0.9787 - val_loss: 0.1213 - val_acc: 0.9596\n",
      "Epoch 260/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0515 - acc: 0.9793 - val_loss: 0.1173 - val_acc: 0.9620\n",
      "Epoch 261/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0522 - acc: 0.9794 - val_loss: 0.1241 - val_acc: 0.9604\n",
      "Epoch 262/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0528 - acc: 0.9790 - val_loss: 0.1235 - val_acc: 0.9599\n",
      "Epoch 263/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0525 - acc: 0.9788 - val_loss: 0.1195 - val_acc: 0.9612\n",
      "Epoch 264/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0521 - acc: 0.9794 - val_loss: 0.1202 - val_acc: 0.9613\n",
      "Epoch 265/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0526 - acc: 0.9788 - val_loss: 0.1147 - val_acc: 0.9627\n",
      "Epoch 266/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0518 - acc: 0.9791 - val_loss: 0.1183 - val_acc: 0.9604\n",
      "Epoch 267/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0522 - acc: 0.9789 - val_loss: 0.1211 - val_acc: 0.9605\n",
      "Epoch 268/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0521 - acc: 0.9791 - val_loss: 0.1215 - val_acc: 0.9599\n",
      "Epoch 269/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0518 - acc: 0.9792 - val_loss: 0.1254 - val_acc: 0.9587\n",
      "Epoch 270/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0518 - acc: 0.9794 - val_loss: 0.1208 - val_acc: 0.9606\n",
      "Epoch 271/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0520 - acc: 0.9791 - val_loss: 0.1201 - val_acc: 0.9620\n",
      "Epoch 272/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0509 - acc: 0.9797 - val_loss: 0.1262 - val_acc: 0.9594\n",
      "Epoch 273/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0518 - acc: 0.9795 - val_loss: 0.1355 - val_acc: 0.9565\n",
      "Epoch 274/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0522 - acc: 0.9793 - val_loss: 0.1217 - val_acc: 0.9611\n",
      "Epoch 275/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0515 - acc: 0.9795 - val_loss: 0.1203 - val_acc: 0.9619\n",
      "Epoch 276/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0509 - acc: 0.9796 - val_loss: 0.1292 - val_acc: 0.9579\n",
      "Epoch 277/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0504 - acc: 0.9799 - val_loss: 0.1169 - val_acc: 0.9616\n",
      "Epoch 278/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0506 - acc: 0.9796 - val_loss: 0.1198 - val_acc: 0.9607\n",
      "Epoch 279/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0513 - acc: 0.9797 - val_loss: 0.1178 - val_acc: 0.9614\n",
      "Epoch 280/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0509 - acc: 0.9797 - val_loss: 0.1235 - val_acc: 0.9599\n",
      "Epoch 281/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0505 - acc: 0.9797 - val_loss: 0.1217 - val_acc: 0.9607\n",
      "Epoch 282/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0505 - acc: 0.9798 - val_loss: 0.1245 - val_acc: 0.9604\n",
      "Epoch 283/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0497 - acc: 0.9801 - val_loss: 0.1193 - val_acc: 0.9615\n",
      "Epoch 284/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0494 - acc: 0.9803 - val_loss: 0.1266 - val_acc: 0.9591\n",
      "Epoch 285/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0513 - acc: 0.9796 - val_loss: 0.1200 - val_acc: 0.9612\n",
      "Epoch 286/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0492 - acc: 0.9803 - val_loss: 0.1217 - val_acc: 0.9605\n",
      "Epoch 287/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0503 - acc: 0.9798 - val_loss: 0.1216 - val_acc: 0.9611\n",
      "Epoch 288/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0505 - acc: 0.9798 - val_loss: 0.1219 - val_acc: 0.9614\n",
      "Epoch 289/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0496 - acc: 0.9801 - val_loss: 0.1338 - val_acc: 0.9577\n",
      "Epoch 290/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0506 - acc: 0.9799 - val_loss: 0.1259 - val_acc: 0.9601\n",
      "Epoch 291/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0497 - acc: 0.9803 - val_loss: 0.1199 - val_acc: 0.9622\n",
      "Epoch 292/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0499 - acc: 0.9801 - val_loss: 0.1227 - val_acc: 0.9615\n",
      "Epoch 293/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0500 - acc: 0.9801 - val_loss: 0.1234 - val_acc: 0.9605\n",
      "Epoch 294/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0487 - acc: 0.9807 - val_loss: 0.1203 - val_acc: 0.9617\n",
      "Epoch 295/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0490 - acc: 0.9803 - val_loss: 0.1243 - val_acc: 0.9607\n",
      "Epoch 296/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0494 - acc: 0.9804 - val_loss: 0.1258 - val_acc: 0.9600\n",
      "Epoch 297/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0495 - acc: 0.9803 - val_loss: 0.1220 - val_acc: 0.9608\n",
      "Epoch 298/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0486 - acc: 0.9804 - val_loss: 0.1194 - val_acc: 0.9616\n",
      "Epoch 299/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0497 - acc: 0.9801 - val_loss: 0.1264 - val_acc: 0.9599\n",
      "Epoch 300/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0484 - acc: 0.9807 - val_loss: 0.1230 - val_acc: 0.9614\n",
      "Epoch 301/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0483 - acc: 0.9805 - val_loss: 0.1274 - val_acc: 0.9596\n",
      "Epoch 302/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0487 - acc: 0.9806 - val_loss: 0.1235 - val_acc: 0.9609\n",
      "Epoch 303/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0492 - acc: 0.9805 - val_loss: 0.1272 - val_acc: 0.9604\n",
      "Epoch 304/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0499 - acc: 0.9800 - val_loss: 0.1212 - val_acc: 0.9612\n",
      "Epoch 305/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0487 - acc: 0.9807 - val_loss: 0.1260 - val_acc: 0.9598\n",
      "Epoch 306/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0486 - acc: 0.9808 - val_loss: 0.1230 - val_acc: 0.9612\n",
      "Epoch 307/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0475 - acc: 0.9811 - val_loss: 0.1215 - val_acc: 0.9619\n",
      "Epoch 308/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0487 - acc: 0.9806 - val_loss: 0.1220 - val_acc: 0.9612\n",
      "Epoch 309/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0476 - acc: 0.9809 - val_loss: 0.1305 - val_acc: 0.9585\n",
      "Epoch 310/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0485 - acc: 0.9804 - val_loss: 0.1257 - val_acc: 0.9601\n",
      "Epoch 311/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0482 - acc: 0.9806 - val_loss: 0.1209 - val_acc: 0.9615\n",
      "Epoch 312/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0486 - acc: 0.9807 - val_loss: 0.1263 - val_acc: 0.9601\n",
      "Epoch 313/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0484 - acc: 0.9805 - val_loss: 0.1213 - val_acc: 0.9612\n",
      "Epoch 314/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0479 - acc: 0.9811 - val_loss: 0.1242 - val_acc: 0.9602\n",
      "Epoch 315/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0479 - acc: 0.9808 - val_loss: 0.1240 - val_acc: 0.9607\n",
      "Epoch 316/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0476 - acc: 0.9811 - val_loss: 0.1267 - val_acc: 0.9601\n",
      "Epoch 317/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0477 - acc: 0.9809 - val_loss: 0.1212 - val_acc: 0.9614\n",
      "Epoch 318/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0477 - acc: 0.9810 - val_loss: 0.1255 - val_acc: 0.9604\n",
      "Epoch 319/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0482 - acc: 0.9810 - val_loss: 0.1177 - val_acc: 0.9620\n",
      "Epoch 320/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0469 - acc: 0.9814 - val_loss: 0.1207 - val_acc: 0.9619\n",
      "Epoch 321/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0485 - acc: 0.9807 - val_loss: 0.1229 - val_acc: 0.9610\n",
      "Epoch 322/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0473 - acc: 0.9810 - val_loss: 0.1301 - val_acc: 0.9591\n",
      "Epoch 323/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0471 - acc: 0.9812 - val_loss: 0.1183 - val_acc: 0.9626\n",
      "Epoch 324/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0464 - acc: 0.9814 - val_loss: 0.1258 - val_acc: 0.9604\n",
      "Epoch 325/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0479 - acc: 0.9808 - val_loss: 0.1233 - val_acc: 0.9603\n",
      "Epoch 326/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0473 - acc: 0.9811 - val_loss: 0.1227 - val_acc: 0.9618\n",
      "Epoch 327/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0474 - acc: 0.9812 - val_loss: 0.1222 - val_acc: 0.9611\n",
      "Epoch 328/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0462 - acc: 0.9816 - val_loss: 0.1183 - val_acc: 0.9629\n",
      "Epoch 329/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0471 - acc: 0.9811 - val_loss: 0.1193 - val_acc: 0.9629\n",
      "Epoch 330/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0465 - acc: 0.9815 - val_loss: 0.1203 - val_acc: 0.9624\n",
      "Epoch 331/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0466 - acc: 0.9813 - val_loss: 0.1223 - val_acc: 0.9618\n",
      "Epoch 332/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0461 - acc: 0.9816 - val_loss: 0.1252 - val_acc: 0.9607\n",
      "Epoch 333/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0472 - acc: 0.9810 - val_loss: 0.1206 - val_acc: 0.9620\n",
      "Epoch 334/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0464 - acc: 0.9816 - val_loss: 0.1229 - val_acc: 0.9616\n",
      "Epoch 335/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0460 - acc: 0.9816 - val_loss: 0.1228 - val_acc: 0.9613\n",
      "Epoch 336/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0460 - acc: 0.9817 - val_loss: 0.1191 - val_acc: 0.9629\n",
      "Epoch 337/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0466 - acc: 0.9814 - val_loss: 0.1220 - val_acc: 0.9616\n",
      "Epoch 338/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0468 - acc: 0.9812 - val_loss: 0.1219 - val_acc: 0.9619\n",
      "Epoch 339/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0462 - acc: 0.9815 - val_loss: 0.1233 - val_acc: 0.9615\n",
      "Epoch 340/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0459 - acc: 0.9816 - val_loss: 0.1296 - val_acc: 0.9599\n",
      "Epoch 341/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0479 - acc: 0.9810 - val_loss: 0.1212 - val_acc: 0.9614\n",
      "Epoch 342/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0450 - acc: 0.9819 - val_loss: 0.1205 - val_acc: 0.9632\n",
      "Epoch 343/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0462 - acc: 0.9815 - val_loss: 0.1232 - val_acc: 0.9611\n",
      "Epoch 344/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0456 - acc: 0.9818 - val_loss: 0.1256 - val_acc: 0.9618\n",
      "Epoch 345/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0459 - acc: 0.9819 - val_loss: 0.1201 - val_acc: 0.9624\n",
      "Epoch 346/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0455 - acc: 0.9819 - val_loss: 0.1234 - val_acc: 0.9617\n",
      "Epoch 347/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0448 - acc: 0.9821 - val_loss: 0.1288 - val_acc: 0.9603\n",
      "Epoch 348/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0471 - acc: 0.9814 - val_loss: 0.1229 - val_acc: 0.9620\n",
      "Epoch 349/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0457 - acc: 0.9817 - val_loss: 0.1239 - val_acc: 0.9619\n",
      "Epoch 350/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0457 - acc: 0.9817 - val_loss: 0.1205 - val_acc: 0.9624\n",
      "Epoch 351/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0447 - acc: 0.9821 - val_loss: 0.1230 - val_acc: 0.9616\n",
      "Epoch 352/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0459 - acc: 0.9817 - val_loss: 0.1214 - val_acc: 0.9618\n",
      "Epoch 353/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0456 - acc: 0.9820 - val_loss: 0.1290 - val_acc: 0.9593\n",
      "Epoch 354/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0444 - acc: 0.9822 - val_loss: 0.1227 - val_acc: 0.9624\n",
      "Epoch 355/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0449 - acc: 0.9821 - val_loss: 0.1261 - val_acc: 0.9607\n",
      "Epoch 356/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0462 - acc: 0.9814 - val_loss: 0.1210 - val_acc: 0.9626\n",
      "Epoch 357/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0452 - acc: 0.9821 - val_loss: 0.1230 - val_acc: 0.9616\n",
      "Epoch 358/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0444 - acc: 0.9823 - val_loss: 0.1241 - val_acc: 0.9615\n",
      "Epoch 359/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0443 - acc: 0.9824 - val_loss: 0.1261 - val_acc: 0.9610\n",
      "Epoch 360/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0447 - acc: 0.9823 - val_loss: 0.1196 - val_acc: 0.9624\n",
      "Epoch 361/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0445 - acc: 0.9821 - val_loss: 0.1231 - val_acc: 0.9623\n",
      "Epoch 362/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0442 - acc: 0.9824 - val_loss: 0.1269 - val_acc: 0.9610\n",
      "Epoch 363/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0449 - acc: 0.9819 - val_loss: 0.1230 - val_acc: 0.9618\n",
      "Epoch 364/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0443 - acc: 0.9824 - val_loss: 0.1269 - val_acc: 0.9611\n",
      "Epoch 365/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0443 - acc: 0.9824 - val_loss: 0.1275 - val_acc: 0.9612\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95e4cacfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWZcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95e410de48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model several epochs, and test on the validation set. Plot the loss for train and validation sets\n",
    "t_init = time.time()\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "t = time.time()\n",
    "history = model2.fit(train_data, train_labels, batch_size=1000, epochs=2500, \n",
    "                    validation_data=(valid_data, valid_labels), verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "epoch_time = time.time() - t\n",
    "\n",
    "total_time = time.time() - t_init\n",
    "\n",
    "\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"acc\"])\n",
    "plt.plot(history.history[\"val_acc\"])\n",
    "plt.title('model acc')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Build the model - validation loss of 0.12 (hits 0.1199 at some point) after 250 epochs, but should probably \n",
    "# stop a good bit sooner by \n",
    "# the look of the plot. Maybe averaging a bunch of these together will give some improvement. Also, look into\n",
    "# parameter optimization.\n",
    "print('Build model...')\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(512, activation='relu', input_dim=54))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dense(256, activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dense(128, activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dense(64, activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "# model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "model3.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 370000 samples, validate on 100000 samples\n",
      "Epoch 1/2500\n",
      "370000/370000 [==============================] - 6s 17us/step - loss: 0.6234 - acc: 0.7676 - val_loss: 0.9168 - val_acc: 0.6551\n",
      "Epoch 2/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.4318 - acc: 0.8248 - val_loss: 0.4222 - val_acc: 0.8262\n",
      "Epoch 3/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.3696 - acc: 0.8525 - val_loss: 0.3802 - val_acc: 0.8476\n",
      "Epoch 4/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.3280 - acc: 0.8694 - val_loss: 0.3270 - val_acc: 0.8695\n",
      "Epoch 5/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2968 - acc: 0.8817 - val_loss: 0.2979 - val_acc: 0.8812\n",
      "Epoch 6/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2688 - acc: 0.8907 - val_loss: 0.2799 - val_acc: 0.8864\n",
      "Epoch 7/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2508 - acc: 0.8977 - val_loss: 0.2706 - val_acc: 0.8904\n",
      "Epoch 8/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2361 - acc: 0.9042 - val_loss: 0.2539 - val_acc: 0.8970\n",
      "Epoch 9/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2257 - acc: 0.9076 - val_loss: 0.2362 - val_acc: 0.9037\n",
      "Epoch 10/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.2148 - acc: 0.9126 - val_loss: 0.2244 - val_acc: 0.9087\n",
      "Epoch 11/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.2059 - acc: 0.9159 - val_loss: 0.2288 - val_acc: 0.9080\n",
      "Epoch 12/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1981 - acc: 0.9197 - val_loss: 0.2146 - val_acc: 0.9137\n",
      "Epoch 13/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1939 - acc: 0.9212 - val_loss: 0.2094 - val_acc: 0.9164\n",
      "Epoch 14/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1867 - acc: 0.9243 - val_loss: 0.1972 - val_acc: 0.9212\n",
      "Epoch 15/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1815 - acc: 0.9262 - val_loss: 0.1970 - val_acc: 0.9202\n",
      "Epoch 16/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1771 - acc: 0.9279 - val_loss: 0.1975 - val_acc: 0.9212\n",
      "Epoch 17/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1721 - acc: 0.9303 - val_loss: 0.1794 - val_acc: 0.9280\n",
      "Epoch 18/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1683 - acc: 0.9314 - val_loss: 0.1837 - val_acc: 0.9257\n",
      "Epoch 19/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1655 - acc: 0.9325 - val_loss: 0.1866 - val_acc: 0.9248\n",
      "Epoch 20/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1627 - acc: 0.9341 - val_loss: 0.1853 - val_acc: 0.9267\n",
      "Epoch 21/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1590 - acc: 0.9358 - val_loss: 0.1864 - val_acc: 0.9255\n",
      "Epoch 22/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1560 - acc: 0.9367 - val_loss: 0.1923 - val_acc: 0.9240\n",
      "Epoch 23/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1532 - acc: 0.9377 - val_loss: 0.1697 - val_acc: 0.9328\n",
      "Epoch 24/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1496 - acc: 0.9394 - val_loss: 0.1890 - val_acc: 0.9248\n",
      "Epoch 25/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1484 - acc: 0.9399 - val_loss: 0.1676 - val_acc: 0.9346\n",
      "Epoch 26/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1452 - acc: 0.9410 - val_loss: 0.1660 - val_acc: 0.9337\n",
      "Epoch 27/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1439 - acc: 0.9412 - val_loss: 0.1631 - val_acc: 0.9349\n",
      "Epoch 28/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1412 - acc: 0.9427 - val_loss: 0.1576 - val_acc: 0.9371\n",
      "Epoch 29/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1393 - acc: 0.9436 - val_loss: 0.1632 - val_acc: 0.9350\n",
      "Epoch 30/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1380 - acc: 0.9439 - val_loss: 0.1611 - val_acc: 0.9358\n",
      "Epoch 31/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1349 - acc: 0.9452 - val_loss: 0.1568 - val_acc: 0.9368\n",
      "Epoch 32/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1328 - acc: 0.9460 - val_loss: 0.1488 - val_acc: 0.9409\n",
      "Epoch 33/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1315 - acc: 0.9466 - val_loss: 0.1601 - val_acc: 0.9365\n",
      "Epoch 34/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1295 - acc: 0.9472 - val_loss: 0.1652 - val_acc: 0.9360\n",
      "Epoch 35/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1288 - acc: 0.9476 - val_loss: 0.1710 - val_acc: 0.9327\n",
      "Epoch 36/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1278 - acc: 0.9484 - val_loss: 0.1537 - val_acc: 0.9401\n",
      "Epoch 37/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1262 - acc: 0.9487 - val_loss: 0.1540 - val_acc: 0.9392\n",
      "Epoch 38/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1240 - acc: 0.9499 - val_loss: 0.1453 - val_acc: 0.9433\n",
      "Epoch 39/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1229 - acc: 0.9502 - val_loss: 0.1523 - val_acc: 0.9401\n",
      "Epoch 40/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1223 - acc: 0.9507 - val_loss: 0.1487 - val_acc: 0.9417\n",
      "Epoch 41/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1212 - acc: 0.9507 - val_loss: 0.1519 - val_acc: 0.9403\n",
      "Epoch 42/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1185 - acc: 0.9517 - val_loss: 0.1467 - val_acc: 0.9433\n",
      "Epoch 43/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1190 - acc: 0.9516 - val_loss: 0.1458 - val_acc: 0.9429\n",
      "Epoch 44/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1166 - acc: 0.9526 - val_loss: 0.1456 - val_acc: 0.9440\n",
      "Epoch 45/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1164 - acc: 0.9529 - val_loss: 0.1466 - val_acc: 0.9433\n",
      "Epoch 46/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1147 - acc: 0.9535 - val_loss: 0.1442 - val_acc: 0.9441\n",
      "Epoch 47/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1133 - acc: 0.9542 - val_loss: 0.1455 - val_acc: 0.9429\n",
      "Epoch 48/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1134 - acc: 0.9540 - val_loss: 0.1452 - val_acc: 0.9432\n",
      "Epoch 49/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1116 - acc: 0.9547 - val_loss: 0.1511 - val_acc: 0.9414\n",
      "Epoch 50/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1116 - acc: 0.9548 - val_loss: 0.1398 - val_acc: 0.9453\n",
      "Epoch 51/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1106 - acc: 0.9550 - val_loss: 0.1391 - val_acc: 0.9458\n",
      "Epoch 52/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1094 - acc: 0.9556 - val_loss: 0.1579 - val_acc: 0.9386\n",
      "Epoch 53/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1090 - acc: 0.9557 - val_loss: 0.1481 - val_acc: 0.9422\n",
      "Epoch 54/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1077 - acc: 0.9561 - val_loss: 0.1440 - val_acc: 0.9442\n",
      "Epoch 55/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1074 - acc: 0.9565 - val_loss: 0.1433 - val_acc: 0.9455\n",
      "Epoch 56/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1060 - acc: 0.9574 - val_loss: 0.1369 - val_acc: 0.9471\n",
      "Epoch 57/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1045 - acc: 0.9575 - val_loss: 0.1373 - val_acc: 0.9470\n",
      "Epoch 58/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1042 - acc: 0.9577 - val_loss: 0.1339 - val_acc: 0.9488\n",
      "Epoch 59/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1034 - acc: 0.9582 - val_loss: 0.1442 - val_acc: 0.9440\n",
      "Epoch 60/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1025 - acc: 0.9584 - val_loss: 0.1372 - val_acc: 0.9474\n",
      "Epoch 61/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1025 - acc: 0.9582 - val_loss: 0.1363 - val_acc: 0.9470\n",
      "Epoch 62/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.1019 - acc: 0.9587 - val_loss: 0.1309 - val_acc: 0.9497\n",
      "Epoch 63/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.1001 - acc: 0.9595 - val_loss: 0.1341 - val_acc: 0.9487\n",
      "Epoch 64/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0998 - acc: 0.9595 - val_loss: 0.1269 - val_acc: 0.9508\n",
      "Epoch 65/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0999 - acc: 0.9591 - val_loss: 0.1313 - val_acc: 0.9501\n",
      "Epoch 66/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0987 - acc: 0.9600 - val_loss: 0.1402 - val_acc: 0.9466\n",
      "Epoch 67/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0981 - acc: 0.9602 - val_loss: 0.1414 - val_acc: 0.9464\n",
      "Epoch 68/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0972 - acc: 0.9606 - val_loss: 0.1404 - val_acc: 0.9467\n",
      "Epoch 69/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0978 - acc: 0.9602 - val_loss: 0.1311 - val_acc: 0.9508\n",
      "Epoch 70/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0951 - acc: 0.9618 - val_loss: 0.1342 - val_acc: 0.9491\n",
      "Epoch 71/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0952 - acc: 0.9613 - val_loss: 0.1435 - val_acc: 0.9452\n",
      "Epoch 72/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0951 - acc: 0.9618 - val_loss: 0.1349 - val_acc: 0.9484\n",
      "Epoch 73/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0944 - acc: 0.9615 - val_loss: 0.1306 - val_acc: 0.9499\n",
      "Epoch 74/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0937 - acc: 0.9619 - val_loss: 0.1287 - val_acc: 0.9506\n",
      "Epoch 75/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0933 - acc: 0.9621 - val_loss: 0.1305 - val_acc: 0.9497\n",
      "Epoch 76/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0928 - acc: 0.9625 - val_loss: 0.1288 - val_acc: 0.9507\n",
      "Epoch 77/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0929 - acc: 0.9624 - val_loss: 0.1363 - val_acc: 0.9485\n",
      "Epoch 78/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0926 - acc: 0.9624 - val_loss: 0.1287 - val_acc: 0.9518\n",
      "Epoch 79/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0913 - acc: 0.9628 - val_loss: 0.1397 - val_acc: 0.9471\n",
      "Epoch 80/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0895 - acc: 0.9637 - val_loss: 0.1308 - val_acc: 0.9508\n",
      "Epoch 81/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0898 - acc: 0.9637 - val_loss: 0.1268 - val_acc: 0.9527\n",
      "Epoch 82/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0897 - acc: 0.9638 - val_loss: 0.1364 - val_acc: 0.9493\n",
      "Epoch 83/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0891 - acc: 0.9639 - val_loss: 0.1241 - val_acc: 0.9538\n",
      "Epoch 84/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0893 - acc: 0.9636 - val_loss: 0.1382 - val_acc: 0.9481\n",
      "Epoch 85/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0887 - acc: 0.9639 - val_loss: 0.1334 - val_acc: 0.9491\n",
      "Epoch 86/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0876 - acc: 0.9643 - val_loss: 0.1246 - val_acc: 0.9531\n",
      "Epoch 87/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0878 - acc: 0.9642 - val_loss: 0.1290 - val_acc: 0.9509\n",
      "Epoch 88/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0872 - acc: 0.9650 - val_loss: 0.1238 - val_acc: 0.9535\n",
      "Epoch 89/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0868 - acc: 0.9647 - val_loss: 0.1273 - val_acc: 0.9517\n",
      "Epoch 90/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0864 - acc: 0.9647 - val_loss: 0.1261 - val_acc: 0.9537\n",
      "Epoch 91/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0849 - acc: 0.9653 - val_loss: 0.1330 - val_acc: 0.9507\n",
      "Epoch 92/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0856 - acc: 0.9654 - val_loss: 0.1244 - val_acc: 0.9534\n",
      "Epoch 93/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0845 - acc: 0.9656 - val_loss: 0.1237 - val_acc: 0.9536\n",
      "Epoch 94/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0845 - acc: 0.9655 - val_loss: 0.1315 - val_acc: 0.9508\n",
      "Epoch 95/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0852 - acc: 0.9652 - val_loss: 0.1276 - val_acc: 0.9527\n",
      "Epoch 96/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0836 - acc: 0.9661 - val_loss: 0.1211 - val_acc: 0.9551\n",
      "Epoch 97/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0842 - acc: 0.9659 - val_loss: 0.1437 - val_acc: 0.9468\n",
      "Epoch 98/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0824 - acc: 0.9665 - val_loss: 0.1277 - val_acc: 0.9531\n",
      "Epoch 99/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0833 - acc: 0.9662 - val_loss: 0.1290 - val_acc: 0.9523\n",
      "Epoch 100/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0816 - acc: 0.9669 - val_loss: 0.1301 - val_acc: 0.9520\n",
      "Epoch 101/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0815 - acc: 0.9669 - val_loss: 0.1306 - val_acc: 0.9518\n",
      "Epoch 102/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0821 - acc: 0.9665 - val_loss: 0.1274 - val_acc: 0.9538\n",
      "Epoch 103/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0813 - acc: 0.9671 - val_loss: 0.1225 - val_acc: 0.9546\n",
      "Epoch 104/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0817 - acc: 0.9668 - val_loss: 0.1254 - val_acc: 0.9542\n",
      "Epoch 105/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0801 - acc: 0.9674 - val_loss: 0.1252 - val_acc: 0.9543\n",
      "Epoch 106/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0800 - acc: 0.9674 - val_loss: 0.1224 - val_acc: 0.9556\n",
      "Epoch 107/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0803 - acc: 0.9673 - val_loss: 0.1258 - val_acc: 0.9539\n",
      "Epoch 108/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0796 - acc: 0.9677 - val_loss: 0.1242 - val_acc: 0.9543\n",
      "Epoch 109/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0782 - acc: 0.9682 - val_loss: 0.1261 - val_acc: 0.9540\n",
      "Epoch 110/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0791 - acc: 0.9679 - val_loss: 0.1207 - val_acc: 0.9560\n",
      "Epoch 111/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0779 - acc: 0.9685 - val_loss: 0.1249 - val_acc: 0.9550\n",
      "Epoch 112/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0787 - acc: 0.9682 - val_loss: 0.1284 - val_acc: 0.9530\n",
      "Epoch 113/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0769 - acc: 0.9684 - val_loss: 0.1181 - val_acc: 0.9565\n",
      "Epoch 114/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0780 - acc: 0.9683 - val_loss: 0.1277 - val_acc: 0.9541\n",
      "Epoch 115/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0777 - acc: 0.9685 - val_loss: 0.1340 - val_acc: 0.9505\n",
      "Epoch 116/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0769 - acc: 0.9686 - val_loss: 0.1284 - val_acc: 0.9528\n",
      "Epoch 117/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0756 - acc: 0.9695 - val_loss: 0.1238 - val_acc: 0.9553\n",
      "Epoch 118/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0760 - acc: 0.9691 - val_loss: 0.1334 - val_acc: 0.9516\n",
      "Epoch 119/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0760 - acc: 0.9692 - val_loss: 0.1257 - val_acc: 0.9545\n",
      "Epoch 120/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0748 - acc: 0.9697 - val_loss: 0.1244 - val_acc: 0.9549\n",
      "Epoch 121/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0745 - acc: 0.9701 - val_loss: 0.1241 - val_acc: 0.9545\n",
      "Epoch 122/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0752 - acc: 0.9693 - val_loss: 0.1245 - val_acc: 0.9553\n",
      "Epoch 123/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0738 - acc: 0.9703 - val_loss: 0.1216 - val_acc: 0.9558\n",
      "Epoch 124/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0753 - acc: 0.9692 - val_loss: 0.1238 - val_acc: 0.9552\n",
      "Epoch 125/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0735 - acc: 0.9701 - val_loss: 0.1202 - val_acc: 0.9568\n",
      "Epoch 126/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0745 - acc: 0.9695 - val_loss: 0.1221 - val_acc: 0.9563\n",
      "Epoch 127/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0749 - acc: 0.9697 - val_loss: 0.1212 - val_acc: 0.9559\n",
      "Epoch 128/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0732 - acc: 0.9706 - val_loss: 0.1294 - val_acc: 0.9537\n",
      "Epoch 129/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0731 - acc: 0.9704 - val_loss: 0.1371 - val_acc: 0.9515\n",
      "Epoch 130/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0731 - acc: 0.9702 - val_loss: 0.1214 - val_acc: 0.9576\n",
      "Epoch 131/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0723 - acc: 0.9709 - val_loss: 0.1301 - val_acc: 0.9533\n",
      "Epoch 132/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0735 - acc: 0.9705 - val_loss: 0.1201 - val_acc: 0.9573\n",
      "Epoch 133/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0727 - acc: 0.9705 - val_loss: 0.1264 - val_acc: 0.9542\n",
      "Epoch 134/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0723 - acc: 0.9705 - val_loss: 0.1222 - val_acc: 0.9562\n",
      "Epoch 135/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0721 - acc: 0.9709 - val_loss: 0.1200 - val_acc: 0.9573\n",
      "Epoch 136/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0718 - acc: 0.9709 - val_loss: 0.1306 - val_acc: 0.9528\n",
      "Epoch 137/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0704 - acc: 0.9713 - val_loss: 0.1193 - val_acc: 0.9581\n",
      "Epoch 138/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0718 - acc: 0.9709 - val_loss: 0.1243 - val_acc: 0.9555\n",
      "Epoch 139/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0705 - acc: 0.9714 - val_loss: 0.1242 - val_acc: 0.9555\n",
      "Epoch 140/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0701 - acc: 0.9719 - val_loss: 0.1221 - val_acc: 0.9567\n",
      "Epoch 141/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0703 - acc: 0.9716 - val_loss: 0.1247 - val_acc: 0.9565\n",
      "Epoch 142/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0703 - acc: 0.9715 - val_loss: 0.1217 - val_acc: 0.9565\n",
      "Epoch 143/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0702 - acc: 0.9715 - val_loss: 0.1296 - val_acc: 0.9542\n",
      "Epoch 144/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0690 - acc: 0.9719 - val_loss: 0.1313 - val_acc: 0.9530\n",
      "Epoch 145/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0690 - acc: 0.9720 - val_loss: 0.1157 - val_acc: 0.9587\n",
      "Epoch 146/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0690 - acc: 0.9720 - val_loss: 0.1205 - val_acc: 0.9570\n",
      "Epoch 147/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0686 - acc: 0.9720 - val_loss: 0.1214 - val_acc: 0.9566\n",
      "Epoch 148/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0686 - acc: 0.9723 - val_loss: 0.1235 - val_acc: 0.9565\n",
      "Epoch 149/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0679 - acc: 0.9727 - val_loss: 0.1236 - val_acc: 0.9556\n",
      "Epoch 150/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0680 - acc: 0.9726 - val_loss: 0.1263 - val_acc: 0.9554\n",
      "Epoch 151/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0675 - acc: 0.9728 - val_loss: 0.1179 - val_acc: 0.9584\n",
      "Epoch 152/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0686 - acc: 0.9725 - val_loss: 0.1201 - val_acc: 0.9575\n",
      "Epoch 153/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0684 - acc: 0.9722 - val_loss: 0.1185 - val_acc: 0.9580\n",
      "Epoch 154/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0667 - acc: 0.9729 - val_loss: 0.1234 - val_acc: 0.9564\n",
      "Epoch 155/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0672 - acc: 0.9727 - val_loss: 0.1186 - val_acc: 0.9583\n",
      "Epoch 156/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0667 - acc: 0.9733 - val_loss: 0.1189 - val_acc: 0.9583\n",
      "Epoch 157/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0667 - acc: 0.9730 - val_loss: 0.1207 - val_acc: 0.9575\n",
      "Epoch 158/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0663 - acc: 0.9732 - val_loss: 0.1260 - val_acc: 0.9553\n",
      "Epoch 159/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0659 - acc: 0.9733 - val_loss: 0.1304 - val_acc: 0.9552\n",
      "Epoch 160/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0658 - acc: 0.9733 - val_loss: 0.1199 - val_acc: 0.9581\n",
      "Epoch 161/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0660 - acc: 0.9733 - val_loss: 0.1187 - val_acc: 0.9582\n",
      "Epoch 162/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0659 - acc: 0.9735 - val_loss: 0.1238 - val_acc: 0.9569\n",
      "Epoch 163/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0657 - acc: 0.9733 - val_loss: 0.1191 - val_acc: 0.9582\n",
      "Epoch 164/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0659 - acc: 0.9734 - val_loss: 0.1212 - val_acc: 0.9569\n",
      "Epoch 165/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0654 - acc: 0.9737 - val_loss: 0.1240 - val_acc: 0.9571\n",
      "Epoch 166/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0645 - acc: 0.9739 - val_loss: 0.1251 - val_acc: 0.9571\n",
      "Epoch 167/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0642 - acc: 0.9738 - val_loss: 0.1203 - val_acc: 0.9581\n",
      "Epoch 168/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0647 - acc: 0.9738 - val_loss: 0.1175 - val_acc: 0.9585\n",
      "Epoch 169/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0642 - acc: 0.9741 - val_loss: 0.1226 - val_acc: 0.9575\n",
      "Epoch 170/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0642 - acc: 0.9741 - val_loss: 0.1261 - val_acc: 0.9563\n",
      "Epoch 171/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0639 - acc: 0.9742 - val_loss: 0.1217 - val_acc: 0.9583\n",
      "Epoch 172/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0647 - acc: 0.9735 - val_loss: 0.1238 - val_acc: 0.9579\n",
      "Epoch 173/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0628 - acc: 0.9747 - val_loss: 0.1208 - val_acc: 0.9576\n",
      "Epoch 174/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0636 - acc: 0.9742 - val_loss: 0.1209 - val_acc: 0.9581\n",
      "Epoch 175/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0645 - acc: 0.9739 - val_loss: 0.1236 - val_acc: 0.9578\n",
      "Epoch 176/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0623 - acc: 0.9750 - val_loss: 0.1147 - val_acc: 0.9606\n",
      "Epoch 177/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0625 - acc: 0.9748 - val_loss: 0.1163 - val_acc: 0.9599\n",
      "Epoch 178/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0627 - acc: 0.9745 - val_loss: 0.1269 - val_acc: 0.9563\n",
      "Epoch 179/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0633 - acc: 0.9743 - val_loss: 0.1221 - val_acc: 0.9568\n",
      "Epoch 180/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0622 - acc: 0.9747 - val_loss: 0.1198 - val_acc: 0.9589\n",
      "Epoch 181/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0631 - acc: 0.9747 - val_loss: 0.1159 - val_acc: 0.9600\n",
      "Epoch 182/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0614 - acc: 0.9749 - val_loss: 0.1204 - val_acc: 0.9588\n",
      "Epoch 183/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0626 - acc: 0.9747 - val_loss: 0.1144 - val_acc: 0.9606\n",
      "Epoch 184/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0621 - acc: 0.9751 - val_loss: 0.1206 - val_acc: 0.9586\n",
      "Epoch 185/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0614 - acc: 0.9753 - val_loss: 0.1244 - val_acc: 0.9569\n",
      "Epoch 186/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0614 - acc: 0.9750 - val_loss: 0.1227 - val_acc: 0.9581\n",
      "Epoch 187/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0616 - acc: 0.9752 - val_loss: 0.1214 - val_acc: 0.9583\n",
      "Epoch 188/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0607 - acc: 0.9756 - val_loss: 0.1201 - val_acc: 0.9592\n",
      "Epoch 189/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0608 - acc: 0.9756 - val_loss: 0.1154 - val_acc: 0.9604\n",
      "Epoch 190/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0609 - acc: 0.9754 - val_loss: 0.1207 - val_acc: 0.9581\n",
      "Epoch 191/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0617 - acc: 0.9752 - val_loss: 0.1216 - val_acc: 0.9579\n",
      "Epoch 192/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0611 - acc: 0.9753 - val_loss: 0.1194 - val_acc: 0.9590\n",
      "Epoch 193/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0607 - acc: 0.9755 - val_loss: 0.1246 - val_acc: 0.9571\n",
      "Epoch 194/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0612 - acc: 0.9753 - val_loss: 0.1221 - val_acc: 0.9582\n",
      "Epoch 195/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0604 - acc: 0.9756 - val_loss: 0.1193 - val_acc: 0.9585\n",
      "Epoch 196/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0603 - acc: 0.9758 - val_loss: 0.1228 - val_acc: 0.9581\n",
      "Epoch 197/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0605 - acc: 0.9757 - val_loss: 0.1262 - val_acc: 0.9572\n",
      "Epoch 198/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0604 - acc: 0.9756 - val_loss: 0.1233 - val_acc: 0.9581\n",
      "Epoch 199/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0588 - acc: 0.9765 - val_loss: 0.1202 - val_acc: 0.9588\n",
      "Epoch 200/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0602 - acc: 0.9757 - val_loss: 0.1201 - val_acc: 0.9588\n",
      "Epoch 201/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0591 - acc: 0.9763 - val_loss: 0.1191 - val_acc: 0.9593\n",
      "Epoch 202/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0587 - acc: 0.9764 - val_loss: 0.1168 - val_acc: 0.9605\n",
      "Epoch 203/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0598 - acc: 0.9758 - val_loss: 0.1252 - val_acc: 0.9578\n",
      "Epoch 204/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0587 - acc: 0.9766 - val_loss: 0.1209 - val_acc: 0.9595\n",
      "Epoch 205/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0588 - acc: 0.9763 - val_loss: 0.1160 - val_acc: 0.9606\n",
      "Epoch 206/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0588 - acc: 0.9764 - val_loss: 0.1176 - val_acc: 0.9598\n",
      "Epoch 207/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0587 - acc: 0.9764 - val_loss: 0.1248 - val_acc: 0.9583\n",
      "Epoch 208/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0590 - acc: 0.9763 - val_loss: 0.1188 - val_acc: 0.9602\n",
      "Epoch 209/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0587 - acc: 0.9762 - val_loss: 0.1282 - val_acc: 0.9566\n",
      "Epoch 210/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0582 - acc: 0.9769 - val_loss: 0.1240 - val_acc: 0.9586\n",
      "Epoch 211/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0581 - acc: 0.9767 - val_loss: 0.1216 - val_acc: 0.9584\n",
      "Epoch 212/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0575 - acc: 0.9767 - val_loss: 0.1245 - val_acc: 0.9577\n",
      "Epoch 213/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0575 - acc: 0.9766 - val_loss: 0.1197 - val_acc: 0.9601\n",
      "Epoch 214/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0577 - acc: 0.9767 - val_loss: 0.1231 - val_acc: 0.9584\n",
      "Epoch 215/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0581 - acc: 0.9767 - val_loss: 0.1207 - val_acc: 0.9591\n",
      "Epoch 216/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0573 - acc: 0.9771 - val_loss: 0.1229 - val_acc: 0.9586\n",
      "Epoch 217/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0570 - acc: 0.9771 - val_loss: 0.1229 - val_acc: 0.9592\n",
      "Epoch 218/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0568 - acc: 0.9770 - val_loss: 0.1180 - val_acc: 0.9599\n",
      "Epoch 219/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0563 - acc: 0.9772 - val_loss: 0.1195 - val_acc: 0.9592\n",
      "Epoch 220/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0568 - acc: 0.9772 - val_loss: 0.1162 - val_acc: 0.9604\n",
      "Epoch 221/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0567 - acc: 0.9770 - val_loss: 0.1201 - val_acc: 0.9598\n",
      "Epoch 222/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0572 - acc: 0.9769 - val_loss: 0.1180 - val_acc: 0.9605\n",
      "Epoch 223/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0565 - acc: 0.9775 - val_loss: 0.1174 - val_acc: 0.9609\n",
      "Epoch 224/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0556 - acc: 0.9774 - val_loss: 0.1181 - val_acc: 0.9600\n",
      "Epoch 225/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0557 - acc: 0.9776 - val_loss: 0.1249 - val_acc: 0.9583\n",
      "Epoch 226/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0562 - acc: 0.9773 - val_loss: 0.1229 - val_acc: 0.9594\n",
      "Epoch 227/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0569 - acc: 0.9773 - val_loss: 0.1198 - val_acc: 0.9596\n",
      "Epoch 228/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0550 - acc: 0.9778 - val_loss: 0.1234 - val_acc: 0.9582\n",
      "Epoch 229/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0556 - acc: 0.9776 - val_loss: 0.1162 - val_acc: 0.9615\n",
      "Epoch 230/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0554 - acc: 0.9776 - val_loss: 0.1234 - val_acc: 0.9592\n",
      "Epoch 231/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0559 - acc: 0.9777 - val_loss: 0.1199 - val_acc: 0.9610\n",
      "Epoch 232/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0551 - acc: 0.9781 - val_loss: 0.1159 - val_acc: 0.9613\n",
      "Epoch 233/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0554 - acc: 0.9776 - val_loss: 0.1210 - val_acc: 0.9599\n",
      "Epoch 234/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0548 - acc: 0.9780 - val_loss: 0.1248 - val_acc: 0.9583\n",
      "Epoch 235/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0557 - acc: 0.9777 - val_loss: 0.1192 - val_acc: 0.9602\n",
      "Epoch 236/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0540 - acc: 0.9784 - val_loss: 0.1196 - val_acc: 0.9603\n",
      "Epoch 237/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0549 - acc: 0.9781 - val_loss: 0.1268 - val_acc: 0.9576\n",
      "Epoch 238/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0559 - acc: 0.9776 - val_loss: 0.1203 - val_acc: 0.9604\n",
      "Epoch 239/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0541 - acc: 0.9782 - val_loss: 0.1225 - val_acc: 0.9593\n",
      "Epoch 240/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0534 - acc: 0.9787 - val_loss: 0.1201 - val_acc: 0.9599\n",
      "Epoch 241/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0538 - acc: 0.9781 - val_loss: 0.1187 - val_acc: 0.9609\n",
      "Epoch 242/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0544 - acc: 0.9783 - val_loss: 0.1228 - val_acc: 0.9594\n",
      "Epoch 243/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0549 - acc: 0.9777 - val_loss: 0.1211 - val_acc: 0.9593\n",
      "Epoch 244/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0536 - acc: 0.9783 - val_loss: 0.1187 - val_acc: 0.9608\n",
      "Epoch 245/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0541 - acc: 0.9780 - val_loss: 0.1247 - val_acc: 0.9586\n",
      "Epoch 246/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0533 - acc: 0.9786 - val_loss: 0.1213 - val_acc: 0.9597\n",
      "Epoch 247/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0539 - acc: 0.9783 - val_loss: 0.1228 - val_acc: 0.9597\n",
      "Epoch 248/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0528 - acc: 0.9787 - val_loss: 0.1223 - val_acc: 0.9598\n",
      "Epoch 249/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0527 - acc: 0.9787 - val_loss: 0.1246 - val_acc: 0.9592\n",
      "Epoch 250/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0537 - acc: 0.9782 - val_loss: 0.1270 - val_acc: 0.9590\n",
      "Epoch 251/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0525 - acc: 0.9790 - val_loss: 0.1194 - val_acc: 0.9605\n",
      "Epoch 252/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0528 - acc: 0.9787 - val_loss: 0.1267 - val_acc: 0.9588\n",
      "Epoch 253/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0538 - acc: 0.9785 - val_loss: 0.1220 - val_acc: 0.9607\n",
      "Epoch 254/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0525 - acc: 0.9790 - val_loss: 0.1224 - val_acc: 0.9599\n",
      "Epoch 255/2500\n",
      "370000/370000 [==============================] - 5s 15us/step - loss: 0.0532 - acc: 0.9788 - val_loss: 0.1240 - val_acc: 0.9597\n",
      "Epoch 256/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0516 - acc: 0.9795 - val_loss: 0.1219 - val_acc: 0.9594\n",
      "Epoch 257/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0529 - acc: 0.9787 - val_loss: 0.1254 - val_acc: 0.9596\n",
      "Epoch 258/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0525 - acc: 0.9789 - val_loss: 0.1205 - val_acc: 0.9604\n",
      "Epoch 259/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0524 - acc: 0.9791 - val_loss: 0.1220 - val_acc: 0.9604\n",
      "Epoch 260/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0525 - acc: 0.9790 - val_loss: 0.1220 - val_acc: 0.9596\n",
      "Epoch 261/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0522 - acc: 0.9791 - val_loss: 0.1277 - val_acc: 0.9591\n",
      "Epoch 262/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0523 - acc: 0.9789 - val_loss: 0.1227 - val_acc: 0.9601\n",
      "Epoch 263/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0524 - acc: 0.9791 - val_loss: 0.1259 - val_acc: 0.9598\n",
      "Epoch 264/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0520 - acc: 0.9790 - val_loss: 0.1259 - val_acc: 0.9587\n",
      "Epoch 265/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0510 - acc: 0.9796 - val_loss: 0.1267 - val_acc: 0.9584\n",
      "Epoch 266/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0517 - acc: 0.9791 - val_loss: 0.1264 - val_acc: 0.9597\n",
      "Epoch 267/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0509 - acc: 0.9797 - val_loss: 0.1208 - val_acc: 0.9601\n",
      "Epoch 268/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0526 - acc: 0.9792 - val_loss: 0.1178 - val_acc: 0.9619\n",
      "Epoch 269/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0513 - acc: 0.9793 - val_loss: 0.1258 - val_acc: 0.9596\n",
      "Epoch 270/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0524 - acc: 0.9794 - val_loss: 0.1209 - val_acc: 0.9599\n",
      "Epoch 271/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0508 - acc: 0.9798 - val_loss: 0.1293 - val_acc: 0.9583\n",
      "Epoch 272/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0520 - acc: 0.9792 - val_loss: 0.1189 - val_acc: 0.9618\n",
      "Epoch 273/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0519 - acc: 0.9793 - val_loss: 0.1231 - val_acc: 0.9595\n",
      "Epoch 274/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0524 - acc: 0.9790 - val_loss: 0.1175 - val_acc: 0.9618\n",
      "Epoch 275/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0515 - acc: 0.9797 - val_loss: 0.1188 - val_acc: 0.9626\n",
      "Epoch 276/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0509 - acc: 0.9797 - val_loss: 0.1243 - val_acc: 0.9596\n",
      "Epoch 277/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0506 - acc: 0.9795 - val_loss: 0.1211 - val_acc: 0.9611\n",
      "Epoch 278/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0508 - acc: 0.9795 - val_loss: 0.1260 - val_acc: 0.9603\n",
      "Epoch 279/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0507 - acc: 0.9794 - val_loss: 0.1188 - val_acc: 0.9617\n",
      "Epoch 280/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0498 - acc: 0.9803 - val_loss: 0.1222 - val_acc: 0.9610\n",
      "Epoch 281/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0501 - acc: 0.9798 - val_loss: 0.1210 - val_acc: 0.9602\n",
      "Epoch 282/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0500 - acc: 0.9798 - val_loss: 0.1195 - val_acc: 0.9610\n",
      "Epoch 283/2500\n",
      "370000/370000 [==============================] - 5s 14us/step - loss: 0.0497 - acc: 0.9801 - val_loss: 0.1261 - val_acc: 0.9592\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0kmhICIinRQKIiLS6f8BIuiHmEl9eu0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95e41c37f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95db1a57f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model several epochs, and test on the validation set. Plot the loss for train and validation sets\n",
    "t_init = time.time()\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "t = time.time()\n",
    "history = model3.fit(train_data, train_labels, batch_size=1000, epochs=2500, \n",
    "                    validation_data=(valid_data, valid_labels), verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "epoch_time = time.time() - t\n",
    "\n",
    "total_time = time.time() - t_init\n",
    "\n",
    "\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"acc\"])\n",
    "plt.plot(history.history[\"val_acc\"])\n",
    "plt.title('model acc')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([[1,2], [3,4]])\n",
    "b=np.array([[1,2], [3,3]])\n",
    "np.sum(np.all(a == b, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370000/370000 [==============================] - 0s 1us/step\n",
      "370000/370000 [==============================] - 1s 2us/step\n",
      "370000/370000 [==============================] - 1s 2us/step\n",
      "370000/370000 [==============================] - 1s 2us/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(train_data, batch_size=10000, verbose=1)\n",
    "preds1 = model1.predict(train_data, batch_size=10000, verbose=1)\n",
    "preds2 = model2.predict(train_data, batch_size=10000, verbose=1)\n",
    "preds3 = model3.predict(train_data, batch_size=10000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99148378378378377"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_preds = (preds + preds1 + preds2 + preds2) / 4.0\n",
    "np.sum(np.all(to_categorical(np.argmax(avg_preds, axis=1), 7) == train_labels, axis=1))/avg_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/100000 [==============================] - 0s 1us/step\n",
      "100000/100000 [==============================] - 0s 1us/step\n",
      "100000/100000 [==============================] - 0s 1us/step\n",
      "100000/100000 [==============================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(valid_data, batch_size=10000, verbose=1)\n",
    "preds1 = model1.predict(valid_data, batch_size=10000, verbose=1)\n",
    "preds2 = model2.predict(valid_data, batch_size=10000, verbose=1)\n",
    "preds3 = model3.predict(valid_data, batch_size=10000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96736999999999995"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_preds = (preds + preds1 + preds2 + preds2) / 4.0\n",
    "np.sum(np.all(to_categorical(np.argmax(avg_preds, axis=1), 7)==valid_labels, axis=1))/avg_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "run() got an unexpected keyword argument 'kernel_initializer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-67359f4962a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     history = modl.fit(train_data, train_labels, batch_size = 1000, epochs = 2500, \n\u001b[0;32m---> 25\u001b[0;31m                     validation_data=(valid_data,valid_labels), verbose = 0, callbacks=[early_stopping])\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmodels_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: run() got an unexpected keyword argument 'kernel_initializer'"
     ]
    }
   ],
   "source": [
    "models_list = []\n",
    "t_init = time.time()\n",
    "for i in np.arange(10):\n",
    "    modl = Sequential()\n",
    "    modl.add(Dense(512, activation='relu', input_dim=54))\n",
    "    modl.add(BatchNormalization())\n",
    "    modl.add(Dense(256, activation='relu'))\n",
    "    modl.add(BatchNormalization())\n",
    "    modl.add(Dense(128, activation='relu'))\n",
    "    modl.add(BatchNormalization())\n",
    "    modl.add(Dense(64, activation='relu'))\n",
    "    modl.add(BatchNormalization())\n",
    "    modl.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    # model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    modl.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "    # train the model several epochs, and test on the validation set. Plot the loss for train and validation sets\n",
    "\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=25)\n",
    "\n",
    "    history = modl.fit(train_data, train_labels, batch_size=1000, epochs=2500, \n",
    "                    validation_data=(valid_data, valid_labels), verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    models_list.append(modl)\n",
    "    print('Model {} created. Current run time = {:0.1f} minutes.'.format(i, (time.time()-t_init) / 60))\n",
    "    del modl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/100000 [==============================] - 0s 1us/step\n",
      "100000/100000 [==============================] - 0s 1us/step\n",
      "100000/100000 [==============================] - 0s 1us/step\n",
      "100000/100000 [==============================] - 0s 1us/step\n",
      "100000/100000 [==============================] - 0s 1us/step\n",
      "100000/100000 [==============================] - 0s 1us/step\n",
      "100000/100000 [==============================] - 0s 1us/step\n",
      "100000/100000 [==============================] - 0s 1us/step\n",
      "100000/100000 [==============================] - 0s 1us/step\n",
      "100000/100000 [==============================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "for i in np.arange(10):\n",
    "    pred_list.append(models_list[i].predict(valid_data, batch_size=1000, verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96872999999999998"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_preds = pred_list[0]\n",
    "for i in np.arange(1, 10):\n",
    "    avg_preds = avg_preds + pred_list[i]\n",
    "avg_preds = avg_preds / 10\n",
    "np.sum(np.all(to_categorical(np.argmax(avg_preds, axis=1), 7) == valid_labels, axis=1)) / avg_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111012/111012 [==============================] - 0s 1us/step\n",
      "111012/111012 [==============================] - 0s 1us/step\n",
      "111012/111012 [==============================] - 0s 1us/step\n",
      "111012/111012 [==============================] - 0s 1us/step\n",
      "111012/111012 [==============================] - 0s 1us/step\n",
      "111012/111012 [==============================] - 0s 1us/step\n",
      "111012/111012 [==============================] - 0s 1us/step\n",
      "111012/111012 [==============================] - 0s 1us/step\n",
      "111012/111012 [==============================] - 0s 1us/step\n",
      "111012/111012 [==============================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "for i in np.arange(10):\n",
    "    pred_list.append(models_list[i].predict(test_data, batch_size=1000, verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96688646272475043"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_preds = pred_list[0]\n",
    "for i in np.arange(1, 10):\n",
    "    avg_preds = avg_preds + pred_list[i]\n",
    "avg_preds = avg_preds / 10\n",
    "np.sum(np.all(to_categorical(np.argmax(avg_preds, axis=1), 7) == test_labels, axis=1)) / avg_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "`save_weights` requires h5py.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-17c840b99ddb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodels_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/ground_cover_classifier_natural_deep_{}.h5'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`save_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m         \u001b[0;31m# If file exists and should not be overwritten:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverwrite\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: `save_weights` requires h5py."
     ]
    }
   ],
   "source": [
    "for i in np.arange(10):\n",
    "    models_list[i].save('models/ground_cover_classifier_natural_deep_{}.h5'.format(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
